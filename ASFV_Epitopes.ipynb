{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASFV_Epitopes",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6r0bSY2ordJKr5MuOUXZo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ros-luc/ASFV-epitopes/blob/main/ASFV_Epitopes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQtCn7rd-fsE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **0. Preparation**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw_txhNCc3J6"
      },
      "source": [
        "This notebook is prepared to run in Google Colaboratory, and it will install several packages. \n",
        "\n",
        "\n",
        "*In case of running locally, please be advised that many of these steps **are NOT needed** and might **cause problems** in your system.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7lOpfsIdQef"
      },
      "source": [
        "**Prerequisites for this pipeline:**\n",
        "\n",
        "Python packages:\n",
        "\n",
        "*   Biopython\n",
        "*   Selenium\n",
        "*   Pandas\n",
        "*   Numpy\n",
        "*   tqdm\n",
        "*   lxml\n",
        "*   Scipy\n",
        "*   scikit-learn \n",
        "\n",
        "Additional software:\n",
        "*   CD-HIT\n",
        "*   MUSCLE\n",
        "*   IEDB MHC I, MHC II and B-cell prediction tools\n",
        "*   NetChop\n",
        "*   NetSurfP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnIkvHULUTwE"
      },
      "source": [
        "### Declaring base variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0kTfWSsUbIo"
      },
      "source": [
        "#These variables are specific to the organism we are studying\n",
        "\n",
        "#UniProt proteomes query\n",
        "UNIPROT_QUERY = \"ASFV\"\n",
        "\n",
        "#In case we want a particular proteome to be present in all clusters, we can\n",
        "#specify it here. Otherwise, leave it as an empty string.\n",
        "mustHaveProteome = \"UP000141072\"\n",
        "\n",
        "#We need the ID of our organism at IEDB. For example, for ASFV is 10497; for\n",
        "#T. cruzi is 5693. It can be found at IEDB.org and searching in \"Organism\".\n",
        "IEDB_ID = 10497"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Hlx3Md1fRc"
      },
      "source": [
        "### Making base directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTlDKJFG5pwW"
      },
      "source": [
        "!mkdir -p ./RV/{DATA/{RAW,PROCESSED/{\"T_PREDICTIONS\",\"B_PREDICTIONS\"}},TOOLS}\n",
        "%cd /content/RV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ2o95-cAhBG"
      },
      "source": [
        "### Installing Miniconda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZM5yWsFXvLP"
      },
      "source": [
        "We are installing **version 4.5.4**, since it runs on **Python 3.6**.\n",
        "\n",
        "**All other packages that we install have to be for Python 3.6 in order to avoid conflicts.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c2ZnVPA66VY"
      },
      "source": [
        "!which python3\n",
        "!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSYjUJZa803S"
      },
      "source": [
        "%%shell\n",
        "wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "chmod +x Miniconda3-4.5.4-Linux-x86_64.sh #We have to make the installer executable\n",
        "bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "rm \"/content/RV/Miniconda3-4.5.4-Linux-x86_64.sh\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqf3436_De_w"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwS5lrCC5TtL"
      },
      "source": [
        "!which python3\n",
        "!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrys_UGkG31w"
      },
      "source": [
        "### Installing Biopython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aObfYKHK8zmw"
      },
      "source": [
        "Biopython includes many useful tools to parse biological sequences and results from tools such as BLAST or MSAs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_rgxtlotx6q"
      },
      "source": [
        "!conda install -c conda-forge biopython python=3.6.5 -y -q "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx2GcIxX4KB1"
      },
      "source": [
        "### Installing and configuring Selenium"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OalH41JPuyIr"
      },
      "source": [
        "!conda install selenium python=3.6.5 -y -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Gp5uvF4N3w"
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8v-IfnH4N30"
      },
      "source": [
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUXkFKsS_97e"
      },
      "source": [
        "### Installing CD-HIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_ze8rgD3PMP"
      },
      "source": [
        "!conda install -c bioconda cd-hit python=3.6.5 -y -q "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmK50KXD_0au"
      },
      "source": [
        "### Installing MUSCLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1xcJQRU9iXO"
      },
      "source": [
        "%%shell\n",
        "wget https://www.drive5.com/muscle/downloads3.8.31/muscle3.8.31_i86linux64.tar.gz\n",
        "tar -xzvf muscle3.8.31_i86linux64.tar.gz\n",
        "cp muscle3.8.31_i86linux64 /usr/local/bin #We copy it here so that it can run properly\n",
        "chmod 755 /usr/local/bin/muscle3.8.31_i86linux64 #We change its permissions too.\n",
        "\n",
        "rm \"/content/RV/muscle3.8.31_i86linux64\"\n",
        "rm \"/content/RV/muscle3.8.31_i86linux64.tar.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB7zdrTFaMhv"
      },
      "source": [
        "### Additional packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PUbPT9maQht"
      },
      "source": [
        "Some of these are needed for immunoinformatics tools."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAXSUUA2aLuA"
      },
      "source": [
        "!sudo apt-get -qq install tcsh\n",
        "!sudo apt-get -qq install gawk\n",
        "!sudo apt-get -qq install gfortran\n",
        "!sudo apt-get -qq install csh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGitXz5OA7BU"
      },
      "source": [
        "### Importing Python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npx2-_RaA6nR"
      },
      "source": [
        "#General use\n",
        "import os #To work with files and directories\n",
        "import shutil #To move files and directories\n",
        "import requests #To work with html data\n",
        "from lxml import html #To work with html data\n",
        "import time #Time functions\n",
        "from time import sleep #Sleep function for delays\n",
        "from tqdm.notebook import tqdm #Progress bar\n",
        "import subprocess #Command line\n",
        "import csv #To read csv files\n",
        "from multiprocessing.pool import ThreadPool #To run multiprocessing functions\n",
        "from selenium.webdriver.common.action_chains import ActionChains #Needed for Selenium\n",
        "from openpyxl import load_workbook #Work with .xlsx files\n",
        "from io import StringIO\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "#Biopython\n",
        "from Bio import AlignIO\n",
        "from Bio import SeqIO\n",
        "from Bio import pairwise2\n",
        "from Bio.Align.Applications import MuscleCommandline\n",
        "from Bio.Align.AlignInfo import SummaryInfo\n",
        "from Bio.Align import AlignInfo\n",
        "from Bio.Blast import NCBIWWW\n",
        "from Bio.Blast import NCBIXML\n",
        "\n",
        "#Pandas and similar\n",
        "import pandas as pd \n",
        "import scipy as sc\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "\n",
        "#Google drive files\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0p4QWCp5Yk-"
      },
      "source": [
        "#We will also use this function several times\n",
        "\n",
        "def createPath(pathToCreate):\n",
        "    if not os.path.exists(pathToCreate):\n",
        "        os.makedirs(pathToCreate)\n",
        "    return str(pathToCreate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDEWbierAL0l"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **1. Downloading protein sequences**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R88TTI7kWIPc"
      },
      "source": [
        "We will download all protein entries from the proteomes of our target organism. This section looks for proteomes in UniProt and downloads all the corresponding protein sequences.\n",
        "\n",
        "This section is skippable if you already have a \"master\" fasta file with all proteomes and proteins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNQ0ju02xXQv"
      },
      "source": [
        "## **1a. Downloading all Uniprot proteomes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6vmO1qOy9dl"
      },
      "source": [
        "### **List of proteomes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRBDGTuH2d6M"
      },
      "source": [
        "First, we search for all Uniprot proteomes matching our query and download them. Be sure to test this query beforehand to avoid downloading unwanted proteomes.\n",
        "\n",
        "We also want to fill our proteome list with additional features, such as host, collection date, etc...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10pLosxeNGVc"
      },
      "source": [
        "def DownloadProteomes_Uniprot(query):\n",
        "\n",
        "    #First, we will download the list of proteomes and some information about them\n",
        "    print(f\"Getting list of all {query} proteomes...\", end=\"\")\n",
        "    base_url = \"http://www.uniprot.org/proteomes/\"\n",
        "    payload = {\"query\" : query, \"format\" : \"tab\", \"columns\" : \"id,name,organism-id,mnemonic,proteincount\"}\n",
        "    raw_result = requests.get(base_url, params=payload)\n",
        "\n",
        "    if raw_result.ok:\n",
        "        result = StringIO(raw_result.text)\n",
        "        df = pd.read_csv(result, sep=\"\\t\")\n",
        "    else:\n",
        "        print(\"Something went wrong: \", result_proteome.status_code)\n",
        "    print(\"done!\")\n",
        "    \n",
        "    #We want the accession number of these proteomes to get some more info\n",
        "    print(\"Getting accession numbers...\", end=\"\")\n",
        "    accessionDict = {}\n",
        "    for proteome in tqdm(df[\"Proteome ID\"]):\n",
        "        url = f'https://www.uniprot.org/proteomes/{proteome}'\n",
        "        path = '//*[@id=\"results\"]/tbody/tr/td[3]/div/div[1]/a'\n",
        "        response = requests.get(url)\n",
        "        byte_data = response.content\n",
        "        source_code = html.fromstring(byte_data)\n",
        "        tree = source_code.xpath(path)\n",
        "        if not tree:\n",
        "            print(f\"Accession for {proteome} not available\")\n",
        "        else:\n",
        "            accessionDict[str(tree[0].text_content())] = proteome\n",
        "    df[\"Accession number\"] = accessionDict\n",
        "\n",
        "    #In these dictionaries we will store the new data\n",
        "    countryDict = {}\n",
        "    hostDict = {}\n",
        "    strainDict = {}\n",
        "    dateDict = {}\n",
        "    seroDict = {}\n",
        "    isolateDict = {}\n",
        "    isolatezoneDict = {}\n",
        "    \n",
        "    listofdicts = [countryDict,\n",
        "                hostDict,\n",
        "                strainDict,\n",
        "                dateDict,\n",
        "                seroDict,\n",
        "                isolateDict,\n",
        "                isolatezoneDict\n",
        "                ]\n",
        "\n",
        "    print(\"Getting proteome features...\", end=\"\")\n",
        "    for access in tqdm(df[\"Accession number\"]):\n",
        "        \n",
        "        #We give an empty default value to each dictionary\n",
        "        for dictionary in listofdicts:\n",
        "            dictionary[access] = \"\"\n",
        "        \n",
        "        url = f\"https://www.ebi.ac.uk/ena/browser/api/embl/{access}\"\n",
        "        response = requests.get(url)\n",
        "        \n",
        "        if response.ok:\n",
        "            for line in response.iter_lines():\n",
        "                \n",
        "                #Here is a custom parser for getting additional information\n",
        "                if \"host=\" in str(line):\n",
        "                    hostDict[access] = str(line).split(\"host=\")[1].replace('\"',\"\").replace(\"'\",\"\")\n",
        "                    continue  \n",
        "                \n",
        "                if \"/lab_host=\" in str(line):\n",
        "                    hostDict[access] = str(line).split(\"/lab_host=\")[1].replace('\"',\"\").replace(\"'\",\"\")\n",
        "                    continue\n",
        "\n",
        "                if \"/country=\" in str(line):\n",
        "                    countryDict[access] = str(line).split(\"/country=\")[1].replace('\"',\"\").replace(\"'\",\"\")\n",
        "                    continue\n",
        "                \n",
        "                if \"/strain=\" in str(line):\n",
        "                    strainDict[access] = str(line).split(\"/strain=\")[1].replace('\"',\"\").replace(\"'\",\"\")\n",
        "                    continue\n",
        "                \n",
        "                if \"/collection_date=\" in str(line):\n",
        "                    dateDict[access] = str(line).split(\"/collection_date=\")[1].replace('\"',\"\").replace(\"'\",\"\")\n",
        "                    continue\n",
        "\n",
        "                if \"/serotype=\" in str(line):\n",
        "                    seroDict[access] = str(line).split(\"/serotype=\")[1].replace('\"',\"\").replace(\"'\",\"\")\n",
        "                    continue\n",
        "\n",
        "                if \"/isolate=\" in str(line):\n",
        "                    isolateDict[access] = str(line).split(\"/isolate=\")[1].replace('\"',\"\").replace(\"'\",\"\")\n",
        "                    continue\n",
        "\n",
        "                if \"/isolation_source=\" in str(line):\n",
        "                    isolatezoneDict[access] = str(line).split(\"/isolation_source=\")[1].replace('\"',\"\").replace(\"'\",\"\")\n",
        "                    continue\n",
        "\n",
        "    #Finally, we map this new information in the dataframe\n",
        "    df['host'] = df['Accession number'].map(hostDict)\n",
        "    df['date'] = df['Accession number'].map(dateDict)\n",
        "    df['country'] = df['Accession number'].map(countryDict)\n",
        "    df['strain'] = df['Accession number'].map(strainDict)\n",
        "    df['isolate'] = df['Accession number'].map(isolateDict)\n",
        "    df['isolation source'] = df['Accession number'].map(isolatezoneDict)\n",
        "    df['serotype'] = df['Accession number'].map(seroDict)\n",
        "  \n",
        "    print(f\"Downloaded {str(len(df))} {query} proteome entries.\")\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBfZFOM8A0oX"
      },
      "source": [
        "try:\n",
        "    df_proteomes = DownloadProteomes_Uniprot(UNIPROT_QUERY)\n",
        "    df_proteomes.to_excel(\"./DATA/RAW/Proteomes.xlsx\")\n",
        "    df_proteomes\n",
        "except:\n",
        "    raise Exception(\"Unable to download proteomes.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLkG_Zpv7sZa"
      },
      "source": [
        "### **Assessing proteomes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfDFNJcfLOcA"
      },
      "source": [
        "#### Problematic proteomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc6bSpzfLRj4"
      },
      "source": [
        "Some proteomes give aberrant alignments due to being too fragmented or incomplete. This gives problems down the line during the entropy calculations, so we will discard them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4nHjcivLbjH"
      },
      "source": [
        "#These are specific for African Swine Fever Virus. Modify list elements as needed.\n",
        "\n",
        "problematic_proteomes = [\"UP000321214\", #Kyev 2016 (245 proteins)\n",
        "                         \"UP000274966\", #Poland 2016 o23 (17 proteins)\n",
        "                         \"UP000268777\", #Poland 2016 o9 (11 proteins)\n",
        "                         \"UP000282187\", #Poland 2017 C220 (5 proteins)\n",
        "                         \"UP000278405\", #Poland 2016 o10  (3 proteins)\n",
        "                         \"UP000266411\", #Sardinia Sassari 2008 (234 proteins)\n",
        "                         \"UP000423628\", #South Africa 1985/SPEC 57 (74 proteins)\n",
        "                        ]\n",
        "\n",
        "for proteome in problematic_proteomes:\n",
        "    df_proteomes = df_proteomes.drop(df_proteomes.loc[df_proteomes['Proteome ID']==proteome].index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e5mWxV3xn4i"
      },
      "source": [
        "## **1b. Downloading protein sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57MBUxiJ2nJL"
      },
      "source": [
        "We download all the proteins from each proteome, and tag them with their proteome origin. We then append each protein to a new fasta file, which will contain all the proteins from all proteomes.\n",
        "\n",
        "The resulting fasta entries will have the format `>PROTEOME-ID_PROTEIN-ID`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OWlFKinX5NE"
      },
      "source": [
        "def fetch_proteome(proteome):\n",
        "    base_url = \"http://www.uniprot.org/uniprot/\"\n",
        "    payload = {\"query\": \"proteome:\" + proteome,\n",
        "               \"format\": \"fasta\",\n",
        "               \"include\": \"yes\"\n",
        "                }\n",
        "\n",
        "    result = requests.get(base_url, params = payload)\n",
        "    if result.ok:\n",
        "        data = {proteome : result.text}\n",
        "        print(\"Downloaded \" \n",
        "              + result.headers[\"X-Total-Results\"] \n",
        "              + \" proteins for proteome \" \n",
        "              + proteome\n",
        "              )\n",
        "        return data\n",
        "    else:\n",
        "        print(\"Could not download proteome \" + proteome)\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUxWCY3gmWA0"
      },
      "source": [
        "allproteomes = list(df_proteomes[\"Proteome ID\"])\n",
        "\n",
        "for i in range(1,10):\n",
        "    try:\n",
        "        results = ThreadPool(len(allproteomes)).imap_unordered(fetch_proteome, allproteomes)\n",
        "        for proteome in results:\n",
        "            for key, value in proteome.items():\n",
        "                prots = value.split(\">\")[1:]\n",
        "                for entry in prots:\n",
        "                    uniprotID = entry.split(\"\\n\")[0].split(\"|\")[1]\n",
        "                    sequence = \"\".join(entry.split(\"\\n\")[1:])\n",
        "                    with open(\"./DATA/RAW/allproteins.fa\", \"a+\") as f:\n",
        "                        f.write(\">\" + key + \"_\" + uniprotID)\n",
        "                        f.write(\"\\n\")\n",
        "                        f.write(sequence)\n",
        "                        f.write(\"\\n\")\n",
        "        break\n",
        "    except:\n",
        "        print(\"Connection error\")\n",
        "        time.sleep(30)\n",
        "                 \n",
        "print(\"All proteins downloaded!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl4w_fCWBiNb"
      },
      "source": [
        "---\n",
        "# **2. Prediction of T-cell epitopes**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AnPhnGk2pj7"
      },
      "source": [
        "## **2a. Protein clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUJL0EoQ-lOC"
      },
      "source": [
        "### **cd-hit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odJ5WZk2eeLl"
      },
      "source": [
        "We run the program *CD-HIT* to create protein clusters.\n",
        "\n",
        "```\n",
        "-i --> Input file\n",
        "-o --> Output\n",
        "-d 0 --> Full description of cluster name until first space\n",
        "-c --> % identity cutoff threshold (set to 0.8, deafult is 0.9)\n",
        "-s --> % of length for smaller sequences, we set to 0.75 to avoid strange alignments of short proteins (default 0)\n",
        "-n --> word size\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-qfIG7nbRlJ"
      },
      "source": [
        "The input file must be a FASTA file with all the protein sequences to cluster.\n",
        "\n",
        "For this pipeline, each entry has to be in the format `>PROTEOME-ID_PROTEIN-ID`.\n",
        "\n",
        "e.g.:\n",
        "\n",
        "```\n",
        ">UP000000861_P0C9M9\n",
        "MNSLQVLTKKVLIENKAFSNYHEDDSFILQQLGLWWENGPIGFCKQCKMVTGGSMLCSDVDSYELDRALVKAVKENQTDL...\n",
        ">UP000000861_P0C9Q0\n",
        "MLPSLQSLTKKVLAGQCLPEDQHYLLKCYDLWWNNAPITFDHNLRLIKSAGLQEGLDLNMALVKAVKENNYSLIKLFTEW...\n",
        ">UP000000861_P0C9K9\n",
        "MITLYEAAIKTLITHRKQILKHPDSREILLALGLYWNKTHILLKCHECGKISLTGKHSTKCININCLLILAIKKKNKRMV...\n",
        ">UP000000861_P0C8F5\n",
        "MKMHIARDSIVYLLNKHLQNTILTNKIEQECFLQADTPKKYLQYIKPFLINCMTKNITTDLVMKDSKRLEPYITLEMRDI...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c_MQxtsrWCn"
      },
      "source": [
        "#We specify an identity cutoff at 80%, and a minimum length for smaller sequences of 75%\n",
        "\n",
        "!cd-hit -i ./DATA/RAW/allproteins.fa -o \"./DATA/PROCESSED/T_PREDICTIONS/globalclusters\" -d 0 -c 0.8 -s 0.75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsslSx122z2P"
      },
      "source": [
        "### **Cluster filtering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWIJbkv1gxGC"
      },
      "source": [
        "We will filter our clusters according to:\n",
        "\n",
        "1.   **Number of proteomes in each cluster**\n",
        "\n",
        "    We want a minimum representation of all strains in our clusters.\n",
        "\n",
        "\n",
        "2.   **Presence of a particular proteome in each cluster**\n",
        "\n",
        "    In case we want a particular proteome to be present always.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b6uj5JaGlc-"
      },
      "source": [
        "def parseClusters(clstrfile):\n",
        "    returndict = {}\n",
        "    print(f\"Parsing clusters for file {os.path.basename(clstrfile)}:\")\n",
        "\n",
        "    with open(clstrfile, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                clusterID = line.strip()\n",
        "                returndict[clusterID] = []\n",
        "            else:\n",
        "                returndict[clusterID].append(line.strip())\n",
        "    \n",
        "    print(f\"Total number of clusters: {str(len(returndict))}\")\n",
        "\n",
        "    return returndict\n",
        "\n",
        "def filterClusters(clstrfile,\n",
        "                   min_proteomes, #Minimum number of proteomes in each cluster\n",
        "                   mustProteome=\"\"\n",
        "                   ):\n",
        "    \n",
        "    clusterDict = parseClusters(clstrfile)\n",
        "  \n",
        "    for key, value in list(clusterDict.items()): #we have to make it a list so we can delete keys\n",
        "        proteomes = []\n",
        "        for i in value:\n",
        "            proteomes.append(i.split()[2].split(\"_\")[0][1:])\n",
        "\n",
        "        if len(proteomes) < min_proteomes:\n",
        "            try:\n",
        "                del clusterDict[key]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if mustHaveProteome:\n",
        "            if not mustHaveProteome in proteomes:\n",
        "                try:\n",
        "                    del clusterDict[key]\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    print(\"Filtered number of clusters: \" + str(len(clusterDict)))\n",
        "\n",
        "    return clusterDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5NpOBqGJCMy"
      },
      "source": [
        "global_clusters = filterClusters(\"./DATA/PROCESSED/T_PREDICTIONS/globalclusters.clstr\",\n",
        "                                 min_proteomes=14,\n",
        "                                 mustProteome=mustHaveProteome\n",
        "                                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsr7D0Qd2__T"
      },
      "source": [
        "### **Reconstructing FASTAS**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny_iiIuZhVmp"
      },
      "source": [
        "Now we build new FASTA files from these clusters.\n",
        "\n",
        "We are also going to define a list with proteomes to give priority to in the MSAs, and add a prefix to each protein accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayydoGx7JUAs"
      },
      "source": [
        "#In this case, we will give priority to proteomes from genotype II, the current\n",
        "#responsible of ASFV epidemic. We will give two tiers of importance: first \n",
        "#Georgia, and then the rest of genotype II.\n",
        "\n",
        "priorityProteomesList = [\"UP000141072\", #Georgia 2007/1\n",
        "                    \"UP000345145\", #Georgia 2008/2\n",
        "                    \"UP000267045\", #Estonia 2014\n",
        "                    \"UP000326051\", #Lithuania 2014 \n",
        "                    \"UP000325567\", #Moldova 2017\n",
        "                    \"UP000327056\", #CzechRepublic 2017/1\n",
        "                    \"UP000290386\", #China/2018/AnhuiXCGQ\n",
        "                    \"UP000291821\", #China Pig/HLJ/2018\n",
        "                    \"UP000292678\", #China DB/LN/2018\n",
        "                    \"UP000307568\", #Belgium 2018/1\n",
        "                    \"UP000316600\", #China ASFV-wbBS01\n",
        "                    \"UP000324915\", #Belgium Etalle 2018\n",
        "                    \"UP000428265\", #Hungary ASFV_HU_2018\n",
        "                    \"UP000422299\", #China/CAS19-01/2019 \n",
        "                    ]\n",
        "\n",
        "priorityProteomesDict = {}\n",
        "for proteome in priorityProteomesList:\n",
        "    if proteome == mustHaveProteome:\n",
        "        priorityProteomesDict[proteome] = \"01\"\n",
        "    else:\n",
        "        priorityProteomesDict[proteome] = \"02\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fpS7ZAZ-G4R"
      },
      "source": [
        "#We will parse the original file with all proteins in order to create a complete\n",
        "#dictionary that contains all sequences in a format \"proteinID : sequence\"\n",
        "\n",
        "with open(\"./DATA/RAW/allproteins.fa\", \"r\") as f:\n",
        "    allProteins = {record.id:str(record.seq) for record in SeqIO.parse(f, \"fasta\")}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zY_jwRxUC1m"
      },
      "source": [
        "def reconstruct_fasta(clusterDictionary, savePath, proteomePriorityDict):\n",
        "    clusterName = os.path.split(os.path.dirname(savePath))[-1]\n",
        "\n",
        "    for cluster, members in clusterDictionary.items():\n",
        "        clusterProteinList = []\n",
        "        for member in members:\n",
        "            clusterProteinList.append(member)\n",
        "        \n",
        "        fileName = f\"{clusterName}_{cluster[1:]}.fa\"\n",
        "        filePath = os.path.join(savePath, fileName) \n",
        "\n",
        "        with open(filePath, \"w+\") as f:\n",
        "            for protein in clusterProteinList:\n",
        "                fullProt = protein[protein.find(\">\") + 1 : protein.find(\"...\")]\n",
        "                protProteome = fullProt.split(\"_\")[0]\n",
        "                protID = fullProt.split(\"_\")[1]\n",
        "\n",
        "                #Here we asign each proteome a priority by adding a prefix\n",
        "                if protProteome in proteomePriorityDict:\n",
        "                    f.write(f\">{proteomePriorityDict[protProteome]}_{protProteome}_{protID}\\n\")\n",
        "                else:\n",
        "                    f.write(f\">99_{protProteome}_{protID}\\n\")\n",
        "\n",
        "                f.write(allProteins[fullProt])\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "    print (f\"FASTAS for {clusterName} reconstructed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fldBh10iq7T"
      },
      "source": [
        "globalPath = createPath(\"./DATA/PROCESSED/T_PREDICTIONS/FASTAS_CLUSTERS/GLOBAL/\")\n",
        "reconstruct_fasta(global_clusters, globalPath, priorityProteomesDict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQQ6n0pZFXvr"
      },
      "source": [
        "## **2b. Multiple sequence alignment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-2HKfPsxwnp"
      },
      "source": [
        "### **Running MUSCLE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-gzjEfZjyWu"
      },
      "source": [
        "We run the MUSCLE MSA tool for each cluster. It results in a new MSA fasta file stored in MUSCLE ALIGNMENTS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0Llly3REmsA"
      },
      "source": [
        "def runMUSCLE(infile, outfile):\n",
        "    #Here is where we installed MUSCLE in section 0. Modify as needed.\n",
        "    muscle_exe = r\"/usr/local/bin/muscle3.8.31_i86linux64\"\n",
        "\n",
        "    muscle_cline = MuscleCommandline(muscle_exe,\n",
        "                                     input=infile,\n",
        "                                     out=outfile,\n",
        "                                     clwstrict=True\n",
        "                                    )\n",
        "    muscle_cline()\n",
        "\n",
        "def alignDir(fastas_dir):\n",
        "    dirname = os.path.basename(os.path.normpath(fastas_dir))\n",
        "\n",
        "    print(f\"Aligning {dirname} clusters\")\n",
        "    print(\"-\"*len(f\"Aligning {dirname} clusters\"))\n",
        "\n",
        "    out_path = createPath(f\"./DATA/PROCESSED/T_PREDICTIONS/MUSCLE_ALIGNMENTS/UNSORTED/{dirname}\")\n",
        "\n",
        "    for root, dirs, files in os.walk(fastas_dir):\n",
        "        for fasta in tqdm(files):\n",
        "            if fasta.endswith(\".fa\"):\n",
        "                in_file = os.path.join(root, fasta)\n",
        "                out_file = out_path + \"/MSA_\" + fasta\n",
        "                runMUSCLE(in_file, out_file)\n",
        "    print(\"-\"*len(f\"All MSAs for {str(dirname)} done!\"))    \n",
        "    print(f\"All MSAs for {str(dirname)} done!\")\n",
        "    print(\"-\"*len(f\"All MSAs for {str(dirname)} done!\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilVgMCZsFB6Z"
      },
      "source": [
        "for root, dirs, files in os.walk(\"./DATA/PROCESSED/T_PREDICTIONS/FASTAS_CLUSTERS/\"):\n",
        "    for dir in dirs:\n",
        "        if not dir.startswith(\".\"):\n",
        "            alignDir(os.path.join(root, dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLx-tV_0x4Qk"
      },
      "source": [
        "### **Sorting the MSAs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKK1OWesk7ab"
      },
      "source": [
        "We will now open each MSA and sort it first according to our proteome priority, and then by number of positions aligned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4z6gGsRJw3y"
      },
      "source": [
        "def keyMSA(item):\n",
        "    #We need a custom function to sort the MSA by priority and length of alignment\n",
        "    returnList = []\n",
        "    returnList.append(1 - int(item.id.split(\"_\")[0])) #(1 - x) because descending order\n",
        "    returnList.append(len(item) - item.seq.count(\"-\"))\n",
        "    return tuple(returnList)\n",
        "\n",
        "def sortMSA(msafile):\n",
        "    alignment = AlignIO.read(msafile,\"clustal\")\n",
        "    alignment.sort(key = keyMSA, reverse=True)\n",
        "    return alignment\n",
        "\n",
        "def sortMSA_dir(in_folder):\n",
        "    musclefiles = [os.path.join(in_folder, x) for x in os.listdir(in_folder) if x.endswith(\".fa\")]\n",
        "    clusterName = os.path.basename(os.path.normpath(in_folder))\n",
        "    dir_sorted = createPath(f\"./DATA/PROCESSED/T_PREDICTIONS/MUSCLE_ALIGNMENTS/SORTED/{clusterName}/\")\n",
        "\n",
        "    for m in musclefiles:\n",
        "        aln = sortMSA(m)\n",
        "        out_aln_file = os.path.basename(m).replace(\".fa\", \"_sorted.aln\")\n",
        "        out_aln_path = os.path.join(dir_sorted, out_aln_file)\n",
        "        with open(out_aln_path,\"w+\") as outfile:\n",
        "            AlignIO.write(aln, outfile, \"clustal\")\n",
        "\n",
        "    print(f\"Sorted {str(len(musclefiles))} {clusterName} alignments\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvrFAgEBKgvt"
      },
      "source": [
        "for root, dirs, files in os.walk(\"./DATA/PROCESSED/T_PREDICTIONS/MUSCLE_ALIGNMENTS/UNSORTED\"):\n",
        "    for dir in dirs:\n",
        "        if not dir.startswith(\".\"):\n",
        "            msa_dir = os.path.join(root,dir)\n",
        "            sortMSA_dir(msa_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R38xnJpsw9Zn"
      },
      "source": [
        "## **2c. Entropy and consensus sequence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm4o3jRFcP3x"
      },
      "source": [
        "### **Calculate entropy and generate consensus sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctx4mOp_masC"
      },
      "source": [
        "We will use the Shannon entropy to calculate the variability in each position of each MSA. If a position has an entropy above the selected threshold (by default 0), it will be masked with an asterisk(*), thus generating a masked consensus sequence.\n",
        "\n",
        "Then, we will find the sequence from the MSA that most closely resembles the masked consensus, and mask the same positions as the masked consensus. This way, we end up with a \"real\" masked sequence, not a consensus \"artificial\" one. However, if the selected threshold is 0, all the sequences will resemble the masked consensus equally, in which case priotitized proteomes take preference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZgxUBqETvd8"
      },
      "source": [
        "def shannonEntropy(data):\n",
        "    data_series = pd.Series(data)\n",
        "    counts = data_series.value_counts()\n",
        "    shannon_entropy = scipy.stats.entropy(counts)\n",
        "    return shannon_entropy\n",
        "\n",
        "def Shannon(alnfile, shannon_threshold=0):   \n",
        "    clusterName = os.path.dirname(alnfile).split(\"/\")[-1]\n",
        "    shannon_str = str(shannon_threshold).replace(\".\", \"\")\n",
        "    dest_pref = f\"./DATA/PROCESSED/T_PREDICTIONS/CONSENSUS/SHANNON_{shannon_str}\"\n",
        "    dest_dir = createPath(f\"{dest_pref}/{clusterName}/\")\n",
        "    dest_file = os.path.basename(alnfile).replace(\"sorted.aln\",\n",
        "                                                  f\"consensus{shannon_str}.fasta\"\n",
        "                                                )\n",
        "    \n",
        "    #It will go position by position, calculate the entropy and then create a\n",
        "    #consensus sequence with the most common amino acid, unless the entropy is \n",
        "    #higher than the threshold, in which case it will insert an *.\n",
        "    consensus_list = []\n",
        "    alignment = AlignIO.read(alnfile, \"clustal\")\n",
        "    for i in range(0,alignment.get_alignment_length()):\n",
        "        aa = []\n",
        "        for record in alignment:\n",
        "            aa.append(record.seq[i])\n",
        "        position_entropy = shannonEntropy(aa)\n",
        "        if position_entropy <= shannon_threshold:\n",
        "            consensus_list.append(Counter(aa).most_common()[0][0])\n",
        "        else:\n",
        "            consensus_list.append(\"*\")\n",
        "    consensus_seq = \"\".join(consensus_list)\n",
        "\n",
        "    #To find the closest sequence to the consensus, we will do a pairwise alignment\n",
        "    #between the consensus and all sequences from the MSA. The sequence with highest\n",
        "    #score will be the closest one. In case of equal score, prioritized proteomes\n",
        "    #have advantage.\n",
        "    refSeq = None\n",
        "    highestScore = 0\n",
        "    for record in alignment:\n",
        "        pairwise_score = pairwise2.align.globalxx(record.seq,\n",
        "                                                consensus_seq,\n",
        "                                                score_only=True\n",
        "                                                )\n",
        "        if pairwise_score > highestScore:\n",
        "            highestScore = pairwise_score\n",
        "            refSeq = record\n",
        "        elif pairwise_score == highestScore:\n",
        "            if int(record.id[0:2]) < int(refSeq.id[0:2]):\n",
        "                refSeq = record\n",
        "    \n",
        "    #Finally, we will mask the positions of the closest sequence according to\n",
        "    #the masked consensus sequence.\n",
        "    refSeq_aa = list(refSeq.seq)\n",
        "    masked_ref = [\"*\"] * len(refSeq_aa)\n",
        "    for i in range(0, len(refSeq_aa)):\n",
        "        if consensus_seq[i] != \"*\":\n",
        "            masked_ref[i] = refSeq_aa[i]\n",
        "    \n",
        "    #We write the results in a new fasta file\n",
        "    with open(dest_dir + dest_file, \"w+\") as f:\n",
        "        f.write(f\">{refSeq.id}\\n\")\n",
        "        f.write(\"\".join(masked_ref))\n",
        "\n",
        "    #All results from the same cluster type and Shannon threshold will be appended\n",
        "    #in a file.\n",
        "    mergedFile = (f\"{dest_pref}/{clusterName}_fullconsensus{shannon_str}.fasta\")\n",
        "    with open(mergedFile, \"a+\") as f:\n",
        "        f.write(f\">{refSeq.id}\\n\")\n",
        "        f.write(\"\".join(masked_ref)+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ObUfbuWI3Od"
      },
      "source": [
        "#In case we want to calculate the consensus for different thresholds, we can\n",
        "#put them in a list here:\n",
        "shannon_thresholds = [0,\n",
        "                      #0.25,\n",
        "                      0.5\n",
        "                      ]\n",
        "\n",
        "for s in shannon_thresholds:\n",
        "    for dir in os.listdir(\"./DATA/PROCESSED/T_PREDICTIONS/MUSCLE_ALIGNMENTS/SORTED\"):\n",
        "        if not dir.startswith(\".\"):\n",
        "            cluster_dir = os.path.join(\"./DATA/PROCESSED/T_PREDICTIONS/MUSCLE_ALIGNMENTS/SORTED\", dir)\n",
        "            print(f\"Calculating invariable proteome at Shannon {s} for {dir}\")\n",
        "            for f in tqdm(os.listdir(cluster_dir)):\n",
        "                if f.endswith(\".aln\"):\n",
        "                    Shannon(os.path.join(cluster_dir, f), s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05FZb81qX79i"
      },
      "source": [
        "## **2d. Validation of IEDB T-cell epitopes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fz8KfBTYVY7"
      },
      "source": [
        "### **Downloading IEDB epitopes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szxaa2xQYVY8"
      },
      "source": [
        "We will download the available confirmed epitopes of our organism from IEDB via Selenium."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGs_HMs_M09t"
      },
      "source": [
        "%cd -q \"./DATA/RAW\"\n",
        "try:\n",
        "    wd = webdriver.Chrome('chromedriver', options=options)\n",
        "    wd.get(f\"http://www.iedb.org/sourceOrgId/{IEDB_ID}\")\n",
        "\n",
        "    wd.find_element_by_xpath('//*[@id=\"content\"]/div/div[2]/table/tbody/tr[5]/td[2]/div/a').click() #Click on T assays link\n",
        "    time.sleep(5)\n",
        "\n",
        "    wd.find_element_by_css_selector('#refine_container > div:nth-child(8) > div.search_content > div.ui_input.check.search_bothlinks > div.checkbox.square').click() #Positive assays only\n",
        "    time.sleep(5)\n",
        "\n",
        "    wd.find_element_by_css_selector('#refine_container > button:nth-child(4)').click() #Search\n",
        "    time.sleep(5)\n",
        "\n",
        "    wd.find_element_by_css_selector('#result_carousel > div.carousel_content.active > div > div.contenttable_content.active > div > div:nth-child(1) > div.exportholder').click() #Export results\n",
        "    time.sleep(5)\n",
        "\n",
        "    wd.find_element_by_css_selector(\"div.inwindow_popup.drop_arrow.identifier_container.show > div > div:nth-child(1) > div.exportholder > div.txt\").click() #Popup Export to CSV file\n",
        "    time.sleep(5)\n",
        "\n",
        "    wd.switch_to.window(wd.window_handles[-1]) #Switch to \"download\" page\n",
        "    time.sleep(30)\n",
        "\n",
        "    wd.refresh() #We refresh to prompt the download\n",
        "    time.sleep(2)\n",
        "\n",
        "    wd.close()\n",
        "\n",
        "    %cd -q ../../\n",
        "\n",
        "    for f in os.listdir(\"./DATA/RAW/\"):\n",
        "        if f.endswith(\".zip\"):\n",
        "            if f.startswith(\"tcell\"):\n",
        "                zipname = \"./DATA/RAW/\" + f\n",
        "                !unzip -qq $zipname -d \"./DATA/RAW/\"\n",
        "                os.remove(zipname)\n",
        "\n",
        "    for f in os.listdir(\"./DATA/RAW/\"):\n",
        "        if f.endswith(\".csv\"):\n",
        "            if f.startswith(\"tcell\"):\n",
        "                os.rename(r\"./DATA/RAW/\" + f, r\"./DATA/RAW/T_Cell_IEDB_epitopes.csv\")\n",
        "\n",
        "    IEDB_T_epitopes = {}\n",
        "    with open(\"./DATA/RAW/T_Cell_IEDB_epitopes.csv\",\"r\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)\n",
        "        next(reader)\n",
        "        for line in reader:\n",
        "            IEDB_T_epitopes[line[0]] = (line[11], #Sequence\n",
        "                                        line[15], #Antigen\n",
        "                                        line[17], #Antigen ID\n",
        "                                        line[101], #MHC allele\n",
        "                                        line[102], #MHC class\n",
        "                                        line[84], #Assay 1\n",
        "                                        line[85], #Assay 2\n",
        "                                        line[87]  #Assay 3\n",
        "                                        )\n",
        "    \n",
        "    print(\"IEDB T-cell epitopes succesfully downloaded.\")\n",
        "except:\n",
        "    %cd -q ../../\n",
        "    print(\"IEDB epitopes could not be downloaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR5zTNPmAqgD"
      },
      "source": [
        "### **Identify conserved IEDB epitopes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzgpB_twEp6Z"
      },
      "source": [
        "def IEDB_T_conserved(consensus_file, epitopes_dict):\n",
        "    records = list(SeqIO.parse(consensus_file, \"fasta\"))\n",
        "\n",
        "    conservedEpis = []\n",
        "\n",
        "    for record in records:\n",
        "        sequence = str(record.seq)\n",
        "        for epitope_id, epitope_data in epitopes_dict.items():\n",
        "            if epitope_data[0] in sequence:\n",
        "                epitope_entry = [epitope_data[0],\n",
        "                                 record.id,\n",
        "                                 epitope_data[1],\n",
        "                                 epitope_data[2],\n",
        "                                 f\"Class {epitope_data[4]} ({epitope_data[3]})\",\n",
        "                                 \" \".join(epitope_data[3:])]\n",
        "                if epitope_entry[4] == \"Class  ()\":\n",
        "                    epitope_entry[4] = \"NA\"\n",
        "                conservedEpis.append(tuple(epitope_entry))\n",
        "    \n",
        "    df = pd.DataFrame(conservedEpis,\n",
        "                      columns=['Epitope',\n",
        "                               'Antigen',\n",
        "                               'IEDB_Antigen name',\n",
        "                               'IEDB_Antigen ID',\n",
        "                               'MHC allele',\n",
        "                               'IEDB_Assay'\n",
        "                               ]\n",
        "                      )\n",
        "\n",
        "    clusterN = os.path.basename(consensus_file).split(\"_\")[0]\n",
        "    shannonN = os.path.dirname(consensus_file).split(\"/\")[-1].split(\"_\")[-1]\n",
        "\n",
        "    output_path = createPath(consensus_file.split(\"CONSENSUS\")[0] + \"IEDB/\")\n",
        "\n",
        "    output_path = \"\".join(output_path \n",
        "                          + clusterN\n",
        "                          + \"_\"\n",
        "                          + shannonN\n",
        "                          + \"_IEDB_epis.csv\"\n",
        "                          )\n",
        "\n",
        "    df.to_csv(output_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eVcOoWhbgIg"
      },
      "source": [
        "consensusFolder = \"./DATA/PROCESSED/T_PREDICTIONS/CONSENSUS/\"\n",
        "\n",
        "for shannon in os.listdir(consensusFolder):\n",
        "    for f in os.listdir(consensusFolder + shannon):\n",
        "        if f.endswith(\".fasta\"):\n",
        "            IEDB_T_conserved(os.path.join(consensusFolder + shannon, f), IEDB_T_epitopes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paLfbRKFOsFx"
      },
      "source": [
        "## **2e. *De novo* epitope predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K7P3JWX-iru"
      },
      "source": [
        "### **CD8 - MHC I epitopes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEyJIVhJDiWI"
      },
      "source": [
        "#### **Proteasome predictions**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lOqf5TXu0si"
      },
      "source": [
        "NetChop simulates how proteins are digested in the proteasome, so we will run it against our conserved sequences.\n",
        "\n",
        "First, you need to download the official NetChop Linux .tar.gz package from DTU.\n",
        "\n",
        "https://services.healthtech.dtu.dk/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPyhtiGOKFOd"
      },
      "source": [
        "This pipeline was tested using NetChop version 3.1d for Linux. Other versions might need changes in the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn-pIYe3OPPK"
      },
      "source": [
        "#Upload the .tar.gz file, uncompress and untar it.\n",
        "\n",
        "!mkdir -p ./TOOLS/NETCHOP/\n",
        "from google.colab import files\n",
        "netchop_pack = files.upload()\n",
        "netchop_filename = next(iter(netchop_pack))\n",
        "!tar -xzf {netchop_filename} -C ./TOOLS/NETCHOP\n",
        "!rm {netchop_filename}\n",
        "\n",
        "for dir in os.listdir(\"./TOOLS/NETCHOP/\"):\n",
        "    if not dir.startswith(\".\"):\n",
        "        NetChopDir = f\"./TOOLS/NETCHOP/{dir}\"\n",
        "        fullNetChopPath = f\"./TOOLS/NETCHOP/{dir}/netchop\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbQ9rvsjS_fI"
      },
      "source": [
        "#Modify the netchop script settings following the README file instructions\n",
        "\n",
        "parsedNetChop = []\n",
        "with open(fullNetChopPath, \"r\") as f:\n",
        "    for line in f:\n",
        "        parsedNetChop.append(line.strip(\"\\n\"))\n",
        "\n",
        "for counter, line in enumerate(parsedNetChop):\n",
        "    if line.startswith(\"setenv  NMHOME\"):\n",
        "        parsedNetChop[counter] = f\"setenv  NMHOME  {os.path.abspath(NetChopDir)}\"\n",
        "    if line.startswith(\"        setenv  TMPDIR\"):\n",
        "        parsedNetChop[counter] = f\"        setenv  TMPDIR  {os.path.abspath(NetChopDir)}/tmp\"\n",
        "\n",
        "with open(fullNetChopPath, \"w+\") as f:\n",
        "    for line in parsedNetChop:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "!chmod 755 {fullNetChopPath}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQC4m1XobJye"
      },
      "source": [
        "Now that we have it installed, we can proceed with proteasomal predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj3L_cBqg3EB"
      },
      "source": [
        "def NetChop(in_file):\n",
        "    \n",
        "    shutil.copy(in_file, \"NetChopTemp.fasta\")\n",
        "    \n",
        "    shannon = in_file.split(\"/\")[-3]\n",
        "    cluster = in_file.split(\"/\")[-2]\n",
        "\n",
        "    protein_entry = SeqIO.read(in_file, \"fasta\")\n",
        "    seqID = protein_entry.id\n",
        "    maskSeq = str(protein_entry.seq)\n",
        "\n",
        "    result = subprocess.run([fullNetChopPath, '-t 0.5', '-v 0', 'NetChopTemp.fasta'], stdout=subprocess.PIPE)\n",
        "    netchopOutput=result.stdout.decode().split(\"\\n\")\n",
        "    with open(\"NetChopOutput.txt\", \"w+\") as f:\n",
        "        f.write(result.stdout.decode())\n",
        "    iter_netchopOutput = iter(netchopOutput)\n",
        "\n",
        "    chopValues = {}\n",
        "    time.sleep(0.5)\n",
        "    for line in iter_netchopOutput:\n",
        "        if \"pos\" in line:\n",
        "            line = next(iter_netchopOutput)\n",
        "            line = next(iter_netchopOutput)\n",
        "            while \"----\" not in line:\n",
        "                chopValues[int(line.split()[0])] = line.split()[2]\n",
        "                line = next(iter_netchopOutput)\n",
        "\n",
        "    parsedList = []\n",
        "    for position, aa in enumerate(maskSeq, 1):\n",
        "        if aa == \"*\" or aa == \"-\" or aa == \"X\":\n",
        "            parsedList.append(aa)\n",
        "        else:\n",
        "            if chopValues[position] == \"S\":\n",
        "                parsedList.append(aa + \"_\")\n",
        "            else:\n",
        "                parsedList.append(aa)\n",
        "    parsedSeq = \"\".join(parsedList)\n",
        "\n",
        "    savePath = createPath(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD8/NETCHOP/\" \n",
        "                          + shannon\n",
        "                          + \"/\"\n",
        "                          )\n",
        "    saveFile = f\"{cluster}_chopconsensus{shannon.split('_')[-1]}.fasta\"\n",
        "\n",
        "    with open(savePath + saveFile, \"a+\") as outputfile:\n",
        "        outputfile.write(f\">{seqID}\\n\")\n",
        "        outputfile.write(parsedSeq + \"\\n\")\n",
        "  \n",
        "    os.remove(\"NetChopTemp.fasta\")\n",
        "    os.remove(\"NetChopOutput.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsWDjoq01uWn"
      },
      "source": [
        "def LaunchNetChop(filelist):\n",
        "    #We need a list of all the files to be processed. They need to be individual\n",
        "    #fasta files\n",
        "\n",
        "    print(\"Launching NetChop...\")\n",
        "    print(\"\")\n",
        "\n",
        "    try:\n",
        "        for x in tqdm(filelist):\n",
        "            NetChop(x)\n",
        "    except:\n",
        "        print(\"Something went wrong, retrying...\")\n",
        "        !rm -rf \"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD8/NETCHOP/\"\n",
        "        for x in tqdm(filelist):\n",
        "            NetChop(x)\n",
        "\n",
        "    print(\"Finished!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNhdpYEd2xrz"
      },
      "source": [
        "#We will launch NetChop against the individual masked files\n",
        "consPath = \"./DATA/PROCESSED/T_PREDICTIONS/CONSENSUS/\"\n",
        "maskedConsensusFiles = []\n",
        "for root, dirs, files in os.walk(consPath):\n",
        "    for f in files:\n",
        "        if f.startswith(\"MSA\"):\n",
        "            maskedConsensusFiles.append(os.path.join(root, f))\n",
        "\n",
        "LaunchNetChop(maskedConsensusFiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tV9LgPub__h"
      },
      "source": [
        "#### **Generate peptides of 9 aminoacids**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1aAV-cYh7wt"
      },
      "source": [
        "MHCI presenting peptides usually have a lenght of 9 aminoacids. We will separate the proteasomal results into conserved peptides with a length equal or bigger than 9 aminoacids. Then, we will create all possible 9-mer overlapping epitopes for those sequences larger than 9 amino acids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQmX53Vv37C0"
      },
      "source": [
        "def Peptidator9(in_folder):\n",
        "\n",
        "    shannon = in_folder.split(\"_\")[-1]\n",
        "    choppedFileList = [os.path.join(in_folder, f) for f in os.listdir(in_folder) if f.endswith(\".fasta\")]\n",
        "\n",
        "    for f in choppedFileList:\n",
        "        cluster = f.split(\"/\")[-1].split(\"_\")[0]\n",
        "        consensusDict = {}\n",
        "        with open(f,\"r\") as consensus:\n",
        "            for line in consensus:\n",
        "                if line.startswith(\">\"):\n",
        "                    id = line.strip()\n",
        "                    consensusDict[id] = \"\"\n",
        "                else:\n",
        "                    consensusDict[id] += line.strip()\n",
        "                    \n",
        "        for key, value in consensusDict.items():\n",
        "            fragments = value.replace(\"_\", \" \").replace(\"*\", \" \").replace(\"-\", \" \").split()\n",
        "            consensusDict[key] = [peptide for peptide in fragments if len(peptide) >= 9]\n",
        "\n",
        "        output_filename = f\"{cluster}_{shannon}_peptides9.fasta\"\n",
        "\n",
        "        with open(os.path.join(in_folder, output_filename), \"w+\") as output:\n",
        "            for key, values in sorted(consensusDict.items()):\n",
        "                for counter, value in enumerate(values, 1):\n",
        "                    output.write(f\"{key}_{str(counter)}\\n\")\n",
        "                    output.write(value + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxltbgk-538N"
      },
      "source": [
        "def Nonamerizator(fastafile):\n",
        "    peptides = list(SeqIO.parse(fastafile,\"fasta\"))\n",
        "    fastabasename = os.path.basename(fastafile).replace(\".fasta\", \"_nonamers.fasta\")\n",
        "    output = createPath(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD8/\") + fastabasename\n",
        "    open(output,\"w+\").close()\n",
        "    \n",
        "    for record in peptides:\n",
        "        ninemers = []\n",
        "        for position, aa in enumerate(record.seq):\n",
        "            ninemers.append(record.seq[position:position+9])\n",
        "            ninemers = [x for x in ninemers if len(x)==9]\n",
        "        \n",
        "        for counter, n in enumerate(ninemers,1):\n",
        "            with open(output,\"a+\") as f:\n",
        "                f.write(f\">{str(record.id)}_{str(counter)}\\n\")\n",
        "                f.write(str(n) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2y9JCeHHSmj"
      },
      "source": [
        "for root, dirs, files in os.walk(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD8/NETCHOP/\"):\n",
        "    for dir in dirs:\n",
        "        Peptidator9(os.path.join(root, dir))\n",
        "        for f in os.listdir(os.path.join(root, dir)):\n",
        "            if f.endswith(\"peptides9.fasta\"):\n",
        "                Nonamerizator(os.path.join(root, dir, f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLYOtz-dspBa"
      },
      "source": [
        "#### **TAP predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXqQp-8MsuUf"
      },
      "source": [
        "We will use the server by Louzoun to run the TAP predictions, using Selenium."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4EBJCOn-Tcd"
      },
      "source": [
        "def prepareTAPfile(peptide_dir, dirTAP):\n",
        "    #Epitope predictions are independent of their origin, so we merge all\n",
        "    #9mers into the same file.\n",
        "    output_file = os.path.join(dirTAP, \"TAP_peptides.txt\")\n",
        "    joinedPeptideList = []\n",
        "    fastas_list = [os.path.join(peptide_dir, x) for x in os.listdir(peptide_dir) if x.endswith(\".fasta\")]\n",
        "    for fasta_file in fastas_list:\n",
        "        peptideList = [str(record.seq) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
        "        for x in peptideList:\n",
        "            joinedPeptideList.append(x)\n",
        "    uniquePeptideList = set(joinedPeptideList)\n",
        "\n",
        "    with open(output_file, \"w+\") as f:\n",
        "        for peptide in uniquePeptideList:\n",
        "            f.write(peptide + \"\\n\")\n",
        "    \n",
        "    return output_file\n",
        "\n",
        "def serverTAP(dirTAP, peptideFile):\n",
        "    currentPath = os.path.abspath(os.getcwd())\n",
        "    peptideFileABS = os.path.abspath(peptideFile)\n",
        "    resultsPath = createPath(os.path.join(dirTAP, \"RESULTS\"))\n",
        "    \n",
        "    %cd -q $resultsPath\n",
        "\n",
        "    wd = webdriver.Chrome('chromedriver', options=options)\n",
        "    \n",
        "    print(\"Connecting to TAP binding app...\", end=\"\")\n",
        "    wd.get(\"https://transition-probability.shinyapps.io/TAP_binding_App/\")\n",
        "    time.sleep(20)\n",
        "    print(\"ok.\")\n",
        "    \n",
        "    print(\"Uploading peptides...\", end=\"\")\n",
        "    wd.find_element_by_id(\"file\").send_keys(peptideFileABS)\n",
        "    time.sleep(20)\n",
        "    print(\"ok.\")\n",
        "    \n",
        "    print(\"Downloading predictions...\", end=\"\")\n",
        "    wd.find_element_by_id(\"downloadData\").click()\n",
        "    time.sleep(20)\n",
        "    print(\"ok.\")\n",
        "    wd.quit()\n",
        "    print(\"Predictions done!\")\n",
        "\n",
        "    %cd -q $currentPath\n",
        "\n",
        "    for f in os.listdir(resultsPath):\n",
        "        if f.endswith(\".csv\"):\n",
        "            TAPresults = os.path.join(resultsPath, f)\n",
        "            return TAPresults\n",
        "    return False\n",
        "\n",
        "def parseTAP(tapcsv, peptideFile):\n",
        "    print(\"Parsing file...\", end=\"\")  \n",
        "    TAPdict = {}\n",
        "    \n",
        "    nonamersList = []\n",
        "    with open(peptideFile, \"r\") as pepf:\n",
        "        for line in pepf:\n",
        "            nonamersList.append(line.strip())\n",
        "\n",
        "    with open(tapcsv, \"r\") as tapf:\n",
        "        reader = csv.reader(tapf, delimiter=',', quotechar='\"')\n",
        "        next(reader)\n",
        "        for counter, line in enumerate(reader):\n",
        "            if line == \"\":\n",
        "                break\n",
        "            else:\n",
        "                TAP_prob = float(line[1].replace(\",\",\".\"))\n",
        "                TAP_IC50 = float(line[2].replace(\",\",\".\"))\n",
        "                TAPdict[nonamersList[counter]] = (TAP_prob, TAP_IC50)\n",
        "    print(\"done!\")\n",
        "    \n",
        "    return TAPdict\n",
        "\n",
        "def TAP(peptides_folder):\n",
        "    TAP_folder = createPath(os.path.join(peptides_folder, \"TAP/\"))\n",
        "    \n",
        "    TAP_peptides = prepareTAPfile(peptides_folder, TAP_folder)\n",
        "\n",
        "    results = serverTAP(TAP_folder, TAP_peptides)\n",
        "\n",
        "    if not results:\n",
        "        raise Exception(\"CSV not found.\")\n",
        "    else:\n",
        "        TAP_dict = parseTAP(results, TAP_peptides)\n",
        "    \n",
        "    return TAP_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPKtV8JjYThy"
      },
      "source": [
        "TAPdict = TAP(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD8/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkHXvaW4s6qX"
      },
      "source": [
        "#### **Predicting peptide affinity for MHC I**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQVeKxpG1eFU"
      },
      "source": [
        "We first download the MHC I binding tool from the IEDB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUPawGgtxvl2"
      },
      "source": [
        "mhci_url = \"https://downloads.iedb.org/tools/mhci/3.1/IEDB_MHC_I-3.1.tar.gz\"\n",
        "mhci_filename = mhci_url.split(\"/\")[-1]\n",
        "\n",
        "print(\"Downloading IEDB MHC I tool...\")\n",
        "!wget $mhci_url\n",
        "!mkdir -p ./TOOLS/MHCI/\n",
        "!tar -xzf $mhci_filename -C ./TOOLS/MHCI/\n",
        "!rm $mhci_filename\n",
        "\n",
        "for dir in os.listdir(\"./TOOLS/MHCI/\"):\n",
        "    if not dir.startswith(\".\"):\n",
        "        MHCI_dir = os.path.abspath(f\"./TOOLS/MHCI/{dir}/\")\n",
        "\n",
        "print(\"Configuring MHC I tool...\")\n",
        "%cd -q $MHCI_dir\n",
        "!./configure\n",
        "%cd -q -\n",
        "\n",
        "MHCI_script = f\"{MHCI_dir}src/predict_binding.py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ftitm_3rgzL"
      },
      "source": [
        "Upload a text file with the alleles to be used, one allele per line.\n",
        "\n",
        "E.g.:\n",
        "\n",
        "SLA-1*0101\n",
        "\n",
        "SLA-1*0201\n",
        "\n",
        "SLA-1*0202\n",
        "\n",
        "SLA-1*0401\n",
        "\n",
        "SLA-1*0501\n",
        "\n",
        "SLA-1*0601"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Neg-tAzm5EB"
      },
      "source": [
        "from google.colab import files\n",
        "MHCI_alleles_upload = files.upload()\n",
        "MHCI_alleles_filename = next(iter(MHCI_alleles_upload))\n",
        "\n",
        "listMHCI_alleles = []\n",
        "with open(MHCI_alleles_filename, \"r\") as f:\n",
        "    for line in f:\n",
        "        listMHCI_alleles.append(line.strip())\n",
        "\n",
        "!mv $MHCI_alleles_filename $MHCI_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boXzGbZwaPLm"
      },
      "source": [
        "def JoinCD8Peptides(cd8_dir):\n",
        "    nonamers_files = [os.path.join(cd8_dir, x) for x in os.listdir(cd8_dir) if x.endswith(\"nonamers.fasta\")]\n",
        "\n",
        "    joinedPeptideList = []\n",
        "    for fasta_file in nonamers_files:\n",
        "        peptideList = [str(record.seq) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
        "        for x in peptideList:\n",
        "            joinedPeptideList.append(x)\n",
        "    uniquePeptideList = set(joinedPeptideList)\n",
        "\n",
        "    CD8_peptides_file = os.path.join(cd8_dir, \"ALL_PEPTIDES.fasta\")\n",
        "    with open(CD8_peptides_file, \"w+\") as f:\n",
        "        for peptide in uniquePeptideList:\n",
        "            f.write(f\">{peptide}\\n\")\n",
        "            f.write(peptide + \"\\n\")\n",
        "    \n",
        "    return CD8_peptides_file\n",
        "\n",
        "def MHCI(dir_CD8, allele_list):\n",
        "    start_time = time.time()\n",
        "\n",
        "    peptides_file = JoinCD8Peptides(dir_CD8)\n",
        "    peptide_abspath = os.path.abspath(peptides_file)\n",
        "    saveDir = createPath(os.path.abspath(\"./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD8/MHCI_predictions/\"))\n",
        "    saveFile = os.path.join(saveDir, \"mhci_output.txt\")\n",
        "    \n",
        "    print(\"Calculating MHC I binding affinities\")\n",
        "    print(\"-----------------------------------\")\n",
        "    print(\"This might take a while...\")\n",
        "    print(\"-----------------------------------\")\n",
        "\n",
        "    %cd -q $MHCI_dir\n",
        "    try:\n",
        "        script_arguments = [\"python\",\n",
        "                            \"./src/predict_binding.py\",\n",
        "                            \"IEDB_recommended\",\n",
        "                            \",\".join(allele_list),\n",
        "                            \",\".join(['9' for x in allele_list]),\n",
        "                            f\"'{peptide_abspath}'\",\n",
        "                            \">\",\n",
        "                            f\"'{saveFile}'\",\n",
        "                            ]\n",
        "\n",
        "        mhci_script = \" \".join(script_arguments)\n",
        "        !$mhci_script\n",
        "\n",
        "        %cd -q -\n",
        "\n",
        "        elapsedtime = time.time() - start_time\n",
        "        print(\"\")\n",
        "        print(\"Predictions for MHC I completed!\")\n",
        "        print(\"Total time: \" + time.strftime(\"%H:%M:%S\", time.gmtime(elapsedtime)))\n",
        "        print(\"-------------------------------\")\n",
        "        print(\"\")\n",
        "    except:\n",
        "        %cd -q -\n",
        "        raise Exception(\"MHC I predictions failed.\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX0myx5uuQsV"
      },
      "source": [
        "MHCI(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD8\", listMHCI_alleles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66bd1DKYHoh5"
      },
      "source": [
        "Finally we parse the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG-_pzv4jx3L"
      },
      "source": [
        "def MHCIparser(mhci_results):\n",
        "    with open(mhci_results, \"r+\") as r:\n",
        "        for nline, line in enumerate(r):\n",
        "            if line.startswith(\"allele\"):\n",
        "                skip_rows = nline\n",
        "    df = pd.read_table(mhci_results, skiprows = skip_rows)\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    MHCI_prediction_dict = {}\n",
        "    for counter, peptide in enumerate(df[\"peptide\"]):\n",
        "        if peptide in MHCI_prediction_dict:\n",
        "            MHCI_prediction_dict[peptide].append((df[\"allele\"].iloc[counter], df[\"rank\"].iloc[counter]))\n",
        "        else:\n",
        "            MHCI_prediction_dict[peptide] = [(df[\"allele\"].iloc[counter], df[\"rank\"].iloc[counter])]\n",
        "\n",
        "    return MHCI_prediction_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t2yCnKVj7mj"
      },
      "source": [
        "MHCI_predictions = MHCIparser('./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD8/MHCI_predictions/mhci_output.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YymadF3x_iMb"
      },
      "source": [
        "#### **Merge predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBacrEuYS7ub"
      },
      "source": [
        "def epitopesByCluster(epitopes_dir):\n",
        "\n",
        "    epitopes_files = [os.path.join(epitopes_dir, x) for x in os.listdir(epitopes_dir) if x.endswith(\"peptides9_nonamers.fasta\")]\n",
        "\n",
        "    dictionaryList = []\n",
        "    for infile in epitopes_files:\n",
        "        prefix = os.path.basename(infile).split(\"peptides9_nonamers.fasta\")[0]\n",
        "        values = {str(record.id):str(record.seq) for record in SeqIO.parse(infile, \"fasta\")}\n",
        "        epidict = {prefix : values}\n",
        "        dictionaryList.append(epidict)\n",
        "    \n",
        "    return dictionaryList\n",
        "\n",
        "def fetchUniprotName(protein):\n",
        "    uniprotID = protein.split(\"_\")[2]\n",
        "    url = 'https://www.uniprot.org/uniprot/' + uniprotID\n",
        "    path_name = '//*[@id=\"content-protein\"]/h1'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        byte_data = response.content\n",
        "        source_code = html.fromstring(byte_data)\n",
        "        tree = source_code.xpath(path_name)\n",
        "\n",
        "        if not tree:\n",
        "            return [protein, \"NA\"]\n",
        "        else:\n",
        "            return [protein, str(tree[0].text_content())]\n",
        "    except:\n",
        "        return [protein, \"NA\"]\n",
        "\n",
        "def Uniproter(protlist):    \n",
        "    unique_prots = list(set(protlist))\n",
        "    uniprotDict = {}\n",
        "    results = ThreadPool(20).imap_unordered(fetchUniprotName, unique_prots)\n",
        "    for r in results:\n",
        "        uniprotDict[r[0]] = r[1]\n",
        "    return uniprotDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_sqxyfbNWgA"
      },
      "source": [
        "def MHCI_results(epitopes_dir, MHCIpredictions_dict, TAPpredictions_dict):\n",
        "    savePath = createPath(\"./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD8/rawCSVs\")\n",
        "    epitopes_identities = epitopesByCluster(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD8\")\n",
        "\n",
        "    for cluster_dict in epitopes_identities:\n",
        "        for prefix, epi_data in cluster_dict.items():\n",
        "            print(f\"Merging values for {prefix}\")\n",
        "            print(\"----------------------------\")\n",
        "            dataframe_values = []\n",
        "            print(\"Creating rows...\", end = \"\")\n",
        "            for id, seq in epi_data.items():\n",
        "                for mhci_binding_data in MHCIpredictions_dict[seq]:\n",
        "                    datarow = (seq,\n",
        "                               id, \n",
        "                               TAPpredictions_dict[seq][0], #TAP%\n",
        "                               TAPpredictions_dict[seq][1], #TAPIC50\n",
        "                               mhci_binding_data[0], #allele\n",
        "                               mhci_binding_data[1] #rank\n",
        "                               )\n",
        "                    dataframe_values.append(datarow)\n",
        "            print(\"done!\")\n",
        "\n",
        "            print(\"Building dataframe...\", end = \"\")\n",
        "            df = pd.DataFrame(dataframe_values,\n",
        "                              columns=[\"epitope\",\n",
        "                                       \"protein\",\n",
        "                                       \"TAP_prob\",\n",
        "                                       \"TAP_IC50\",\n",
        "                                       \"MHCI_allele\",\n",
        "                                       \"MHCI_rank\"],\n",
        "                              dtype=float\n",
        "                              )\n",
        "            print(\"done!\")\n",
        "            \n",
        "            print(\"Getting protein names...\", end = \"\")\n",
        "            protein_names = Uniproter([x for x in df[\"protein\"]])\n",
        "            df[\"protein name\"] = df[\"protein\"].map(protein_names)\n",
        "            column_order = [\"epitope\",\n",
        "                            \"protein\",\n",
        "                            \"protein name\",\n",
        "                            \"TAP_prob\",\n",
        "                            \"TAP_IC50\",\n",
        "                            \"MHCI_allele\",\n",
        "                            \"MHCI_rank\"\n",
        "                            ]\n",
        "            df = df[column_order]\n",
        "            print(\"done!\")\n",
        "\n",
        "            print(\"Saving dataframe to csv...\", end = \"\")\n",
        "            df.to_csv(os.path.join(savePath, prefix + \"CD8.csv\"))\n",
        "            print(\"done!\")\n",
        "            print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm1qt0Skb9PZ"
      },
      "source": [
        "MHCI_results(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD8\", MHCI_predictions, TAPdict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JZf1_qsekM9"
      },
      "source": [
        "#### **Formatting results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNOYP4xmHKtc"
      },
      "source": [
        "def FormatCD8(csv_file, MHCI_cutoff=1, TAP_cutoff=1):\n",
        "    savePath = createPath(\"./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD8\")\n",
        "    saveFile = os.path.join(savePath, os.path.basename(csv_file).replace(\".csv\", \".xlsx\"))\n",
        "\n",
        "    df = pd.read_csv(csv_file, index_col=0)\n",
        "\n",
        "    #############################################################\n",
        "    #Filter allele binding percentile. For MHC I, it's usually 1%\n",
        "    df = df[df.MHCI_rank <= MHCI_cutoff]\n",
        "    #############################################################\n",
        "    \n",
        "    ################################\n",
        "    #Filter out all TAP IC50 above 1\n",
        "    df = df[df.TAP_IC50 <= TAP_cutoff]\n",
        "    ################################\n",
        "\n",
        "    alleles_dict = {}\n",
        "    TAP_dict = {}\n",
        "    antigens_dict = {}\n",
        "    epitopes_list = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        epitope = df[\"epitope\"].iloc[i]\n",
        "        \n",
        "        allele = df[\"MHCI_allele\"].iloc[i]\n",
        "        percentile = \"(\" + str(round(df[\"MHCI_rank\"].iloc[i], 2)) + \"%)\"\n",
        "        \n",
        "        TAP_percent = df[\"TAP_prob\"].iloc[i]\n",
        "        TAP_IC50 = df[\"TAP_IC50\"].iloc[i]\n",
        "        \n",
        "        antigen = \"_\".join(df[\"protein\"].iloc[i].split(\"_\")[:3])\n",
        "        antigen_name = df[\"protein name\"].iloc[i]\n",
        "        \n",
        "        #Add only unique epitopes to the list\n",
        "        if not epitope in epitopes_list:\n",
        "            epitopes_list.append(epitope)\n",
        "        \n",
        "        #Each epitope has specific allele percentiles\n",
        "        if epitope in alleles_dict:\n",
        "            alleles_dict[epitope].append(allele + \" \" + percentile)\n",
        "        else:\n",
        "            alleles_dict[epitope] = [allele + \" \" + percentile]\n",
        "        \n",
        "        TAP_dict[epitope] = (TAP_percent, TAP_IC50)\n",
        "\n",
        "        #Each epitope comes from a different protein. Rarely, they can come\n",
        "        #from more than one\n",
        "        if epitope in antigens_dict:\n",
        "            antigens_dict[epitope].append((antigen, antigen_name))\n",
        "        else:\n",
        "            antigens_dict[epitope] = [(antigen, antigen_name)]\n",
        "\n",
        "    for key, values in alleles_dict.items():\n",
        "        unique_alleles = list(set(values)) #Delete repeated alleles\n",
        "        sorted_values = sorted(unique_alleles)\n",
        "        alleles_dict[key] = sorted_values\n",
        "\n",
        "    joined_alleles_dict = {}\n",
        "    for key, values in alleles_dict.items():\n",
        "        total_alleles = len(values)\n",
        "        #We keep the allele, its percentile, and the total alleles binding\n",
        "        joined_alleles_dict[key] = (\" \\n\".join(values), total_alleles)\n",
        "\n",
        "    unique_antigens_dict = {}    \n",
        "    for key, values in antigens_dict.items():\n",
        "        unique_antigens = list(dict.fromkeys(values)) #Delete repeated origins\n",
        "        final_antigens = []\n",
        "        for antigen in unique_antigens: #there might be more than one origin,\n",
        "        #so we have to take into account that possibility\n",
        "            final_antigens.append(antigen[0] + \" (\" + antigen[1] + \")\")\n",
        "        unique_antigens_dict[key] = \"\\n\".join(final_antigens)\n",
        "\n",
        "    tupledEpitopesList = [] #we will use a list of tuples because it's easier to\n",
        "    #transform into a dataframe later\n",
        "    for epitope in epitopes_list:   \n",
        "        epitopeTuple = (epitope, \n",
        "                        unique_antigens_dict[epitope], #origin antigens\n",
        "                        joined_alleles_dict[epitope][0], #alleles binding\n",
        "                        joined_alleles_dict[epitope][1], #total alleles\n",
        "                        round(TAP_dict[epitope][0], 2), #TAP %\n",
        "                        round(TAP_dict[epitope][1], 2), #TAP IC50\n",
        "                        )\n",
        "        tupledEpitopesList.append(epitopeTuple)\n",
        "\n",
        "    final_df = pd.DataFrame(tupledEpitopesList, columns=[\"epitope\",\n",
        "                                                        \"protein\",\n",
        "                                                        \"MHCI_alleles\",\n",
        "                                                        \"MHCI_total\",\n",
        "                                                        \"TAP_prob\",\n",
        "                                                        \"TAP_IC50\",\n",
        "                                                        ]\n",
        "                            )\n",
        "    \n",
        "    final_df.to_excel(saveFile, sheet_name = f\"MHCI <={str(MHCI_cutoff)}% | TAP <={str(TAP_cutoff)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51-ONTZmi8ka"
      },
      "source": [
        "epitopes_csv_dir = \"./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD8/rawCSVs\"\n",
        "csv_files = [os.path.join(epitopes_csv_dir, x) for x in os.listdir(epitopes_csv_dir) if x.endswith(\".csv\")]\n",
        "\n",
        "for csv in csv_files:\n",
        "    FormatCD8(csv, MHCI_cutoff=1, TAP_cutoff=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmPFM4qRl4cz"
      },
      "source": [
        "### **CD4 - MHC II epitopes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl9I8XdCmNgU"
      },
      "source": [
        "#### **Generate peptides of >= 15 amino acids**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTmlcIpUl9OM"
      },
      "source": [
        "def Peptidator15(in_folder):\n",
        "\n",
        "    shannon = in_folder.split(\"/\")[-1].split(\"_\")[-1]\n",
        "    consensusFilesList = [os.path.join(in_folder, f) for f in os.listdir(in_folder) if f.endswith(\".fasta\")]\n",
        "\n",
        "    for i in consensusFilesList:\n",
        "        cluster = i.split(\"/\")[-1].split(\"_\")[0]\n",
        "        consensusDict = {}\n",
        "        with open(i,\"r\") as consensus:\n",
        "            for line in consensus:\n",
        "                if line.startswith(\">\"):\n",
        "                    id = line.strip()\n",
        "                    consensusDict[id] = \"\"\n",
        "                else:\n",
        "                    consensusDict[id] += line.strip()\n",
        "                    \n",
        "        for key, value in consensusDict.items():\n",
        "            fragments = value.replace(\"_\", \" \").replace(\"-\", \" \").replace(\"*\", \" \").replace(\"X\", \" \").split()\n",
        "            consensusDict[key] = [peptide for peptide in fragments if len(peptide) >= 15]\n",
        "\n",
        "\n",
        "        destination = createPath(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD4/\")       \n",
        "        filename = cluster + \"_\" + shannon + \"_peptides15.fasta\"\n",
        "\n",
        "        with open(destination + filename, \"w+\") as output:\n",
        "            for key, values in sorted(consensusDict.items()):\n",
        "                for counter, value in enumerate(values, 1):\n",
        "                    output.write(key + \"_\" + str(counter))\n",
        "                    output.write(\"\\n\")\n",
        "                    output.write(value)\n",
        "                    output.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vD4InbTmZan"
      },
      "source": [
        "for folder in os.listdir(\"./DATA/PROCESSED/T_PREDICTIONS/CONSENSUS\"):\n",
        "    if not folder.startswith(\".\"):\n",
        "        Peptidator15(os.path.join(\"./DATA/PROCESSED/T_PREDICTIONS/CONSENSUS\", folder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q_7Yk2P82PJ"
      },
      "source": [
        "#### **Predicting peptide affinity for MHC II**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih_mY_nu9R7u"
      },
      "source": [
        "mhcii_url = \"https://downloads.iedb.org/tools/mhcii/3.1/IEDB_MHC_II-3.1.tar.gz\"\n",
        "mhcii_filename = mhcii_url.split(\"/\")[-1]\n",
        "\n",
        "print(\"Downloading IEDB MHC II tool...\")\n",
        "!wget $mhcii_url\n",
        "!mkdir -p ./TOOLS/MHCII/\n",
        "!tar -xzf $mhcii_filename -C ./TOOLS/MHCII/\n",
        "!rm $mhcii_filename\n",
        "\n",
        "for dir in os.listdir(\"./TOOLS/MHCII/\"):\n",
        "    if not dir.startswith(\".\"):\n",
        "        MHCII_dir = os.path.abspath(f\"./TOOLS/MHCII/{dir}/\")\n",
        "\n",
        "print(\"Configuring MHC II tool...\")\n",
        "%cd -q $MHCII_dir\n",
        "!./configure.py\n",
        "%cd -q -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "292j2bmUWV5G"
      },
      "source": [
        "MHC_II_alleles_url = \"https://help.iedb.org/hc/en-us/article_attachments/360047583591/hla_ref_set.class_ii.txt\"\n",
        "MHC_II_alleles_file = os.path.basename(MHC_II_alleles_url)\n",
        "\n",
        "!wget $MHC_II_alleles_url\n",
        "\n",
        "listMHCII_alleles = []\n",
        "with open(MHC_II_alleles_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        listMHCII_alleles.append(line.strip())\n",
        "\n",
        "!mv $MHC_II_alleles_file $MHCII_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRjtLsmPBXdG"
      },
      "source": [
        "def JoinCD4Peptides(cd4_dir):\n",
        "    peptide_files = [os.path.join(cd4_dir, x) for x in os.listdir(cd4_dir) if x.endswith(\"peptides15.fasta\")]\n",
        "\n",
        "    joinedPeptideList = []\n",
        "    for fasta_file in peptide_files:\n",
        "        peptideList = [str(record.seq) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
        "        for x in peptideList:\n",
        "            joinedPeptideList.append(x)\n",
        "    uniquePeptideList = set(joinedPeptideList)\n",
        "\n",
        "    CD4_peptides_file = os.path.join(cd4_dir, \"ALL_PEPTIDES.fasta\")\n",
        "    with open(CD4_peptides_file, \"w+\") as f:\n",
        "        for peptide in uniquePeptideList:\n",
        "            f.write(f\">{peptide}\\n\")\n",
        "            f.write(peptide + \"\\n\")\n",
        "    \n",
        "    return CD4_peptides_file\n",
        "\n",
        "def MHCII(dir_CD4, allele_list):\n",
        "    start_time = time.time()\n",
        "\n",
        "    peptides_file = JoinCD4Peptides(dir_CD4)\n",
        "\n",
        "    peptide_abspath = os.path.abspath(peptides_file)\n",
        "    saveDir = createPath(os.path.abspath(\"./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD4/MHCII_predictions/\"))\n",
        "    saveFile = os.path.join(saveDir, \"mhcii_output.txt\")\n",
        "    \n",
        "    print(\"Calculating MHC II binding affinities\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(\"This might take a while...\")\n",
        "    print(\"-------------------------------------\")\n",
        "\n",
        "    %cd -q $MHCII_dir\n",
        "\n",
        "    script_arguments = [\"python\",\n",
        "                        \"mhc_II_binding.py\",\n",
        "                        \"IEDB_recommended\",\n",
        "                        \",\".join(allele_list),\n",
        "                        f\"'{peptide_abspath}'\",\n",
        "                        \">\",\n",
        "                        f\"'{saveFile}'\",\n",
        "                        ]\n",
        "\n",
        "    mhcii_script = \" \".join(script_arguments)\n",
        "    !$mhcii_script\n",
        "\n",
        "    %cd -q -\n",
        "\n",
        "    elapsedtime = time.time() - start_time\n",
        "    print(\"\")\n",
        "    print(\"Predictions for MHC II completed!\")\n",
        "    print(\"Total time: \" + time.strftime(\"%H:%M:%S\", time.gmtime(elapsedtime)))\n",
        "    print(\"-------------------------------\")\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrembKkOBXdK"
      },
      "source": [
        "MHCII(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD4\", listMHCII_alleles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM6ySyzxa-AY"
      },
      "source": [
        "def MHCIIparser(mhcii_results, original_file):\n",
        "    df = pd.read_table(mhcii_results)\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    peptide_dict = {}\n",
        "    with open(original_file, \"r\") as f:\n",
        "        counter = 1\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                peptide_dict[counter] = line.strip()[1:]\n",
        "                counter += 1\n",
        "    \n",
        "    df[\"original_peptide\"] = df[\"seq_num\"].map(peptide_dict)\n",
        "\n",
        "    MHCII_prediction_dict = {}\n",
        "    for counter, o_peptide in enumerate(df[\"original_peptide\"]):\n",
        "        if o_peptide in MHCII_prediction_dict:\n",
        "            MHCII_prediction_dict[o_peptide].append((df[\"peptide\"].iloc[counter], df[\"allele\"].iloc[counter], df[\"consensus_percentile_rank\"].iloc[counter]))\n",
        "        else:\n",
        "            MHCII_prediction_dict[o_peptide] = [(df[\"peptide\"].iloc[counter], df[\"allele\"].iloc[counter], df[\"consensus_percentile_rank\"].iloc[counter])]\n",
        "\n",
        "    return MHCII_prediction_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4UxFZ2ba-Aa"
      },
      "source": [
        "MHCII_predictions = MHCIIparser(\"./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD4/MHCII_predictions/mhcii_output.txt\",\n",
        "                                \"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD4/ALL_PEPTIDES.fasta\"\n",
        "                                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhDtCf0Gvo6a"
      },
      "source": [
        "def clustersMHC_II(fastas_dir):\n",
        "    fileList = [os.path.join(fastas_dir, x) for x in os.listdir(fastas_dir) if x.endswith(\"peptides15.fasta\")]\n",
        "\n",
        "    for cluster_fasta in fileList:\n",
        "        peptide_frags = [(record.id, record.seq) for record in SeqIO.parse(cluster_fasta, \"fasta\")]\n",
        "        row_list = []\n",
        "        for o_peptide in peptide_frags:\n",
        "            origin = \"_\".join(o_peptide[0].split(\"_\")[1:3])\n",
        "            epitopes = MHCII_predictions[o_peptide[1]]\n",
        "            for epitope in epitopes:\n",
        "                row = (epitope[0], #epitope\n",
        "                       origin, #protein\n",
        "                       epitope[1], #allele\n",
        "                       epitope[2], #rank\n",
        "                       )\n",
        "                row_list.append(row)\n",
        "\n",
        "        df = pd.DataFrame(row_list, columns=[\"epitope\",\n",
        "                                             \"protein\",\n",
        "                                             \"MHCII_allele\",\n",
        "                                             \"MHCII_rank\"]\n",
        "                          )\n",
        "        \n",
        "        fileName = os.path.basename(cluster_fasta).replace(\".fasta\", \".csv\")\n",
        "        savePath = os.path.join(\"./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD4\", fileName)\n",
        "        df.to_csv(savePath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWrlQOMkwd3Z"
      },
      "source": [
        "clustersMHC_II(\"./DATA/PROCESSED/T_PREDICTIONS/PEPTIDES/CD4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwS4QmzZrz2z"
      },
      "source": [
        "#### **Formatting results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgEvqhmnkIh4"
      },
      "source": [
        "def joinAlleles_MHC_II(df):\n",
        "    \n",
        "    alleles_dict = {}\n",
        "    antigens_dict = {}\n",
        "    epitopes_list = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        epitope = df[\"epitope\"].iloc[i]\n",
        "        \n",
        "        allele = df[\"MHCII_allele\"].iloc[i]\n",
        "        percentile = f\"({str(round(df['MHCII_rank'].iloc[i], 2))}%)\"\n",
        "        antigen = df[\"protein\"].iloc[i]\n",
        "        \n",
        "        #Add only unique epitopes to the list\n",
        "        if not epitope in epitopes_list:\n",
        "            epitopes_list.append(epitope)\n",
        "        \n",
        "        #Each epitope has specific allele percentiles\n",
        "        if epitope in alleles_dict:\n",
        "            alleles_dict[epitope].append(allele + \" \" + percentile)\n",
        "        else:\n",
        "            alleles_dict[epitope] = [allele + \" \" + percentile]\n",
        "\n",
        "        #Each epitope comes from a different protein. Rarely, they can come\n",
        "        #from more than one\n",
        "        if epitope in antigens_dict:\n",
        "            antigens_dict[epitope].append(antigen)\n",
        "        else:\n",
        "            antigens_dict[epitope] = [antigen]\n",
        "\n",
        "    for key, values in alleles_dict.items():\n",
        "        unique_alleles = list(set(values)) #Delete repeated alleles\n",
        "        sorted_values = sorted(unique_alleles)\n",
        "        alleles_dict[key] = sorted_values\n",
        "\n",
        "    joined_alleles_dict = {}\n",
        "    for key, values in alleles_dict.items():\n",
        "        total_alleles = len(values)\n",
        "        #We keep the allele, its percentile, and the total alleles binding\n",
        "        joined_alleles_dict[key] = (\" \\n\".join(values), total_alleles)\n",
        "\n",
        "    unique_antigens_dict = {}    \n",
        "    for key, values in antigens_dict.items():\n",
        "        unique_antigens = list(set(values)) #Delete repeated origins\n",
        "\n",
        "        if len(unique_antigens) == 1:\n",
        "            unique_antigens_dict[key] = unique_antigens[0]\n",
        "        else: #there might be more than one origin\n",
        "            unique_antigens_dict[key] = \"|\".join(unique_antigens)\n",
        "\n",
        "    tupledEpitopesList = [] #we will use a list of tuples because it's easier to\n",
        "    #transform into a dataframe later\n",
        "    for epitope in epitopes_list:   \n",
        "        epitopeTuple = (epitope,\n",
        "                    joined_alleles_dict[epitope][0], #alleles binding\n",
        "                    joined_alleles_dict[epitope][1], #total alleles\n",
        "                    unique_antigens_dict[epitope], #origin antigens\n",
        "                    )\n",
        "        tupledEpitopesList.append(epitopeTuple)\n",
        "\n",
        "    final_df = pd.DataFrame(tupledEpitopesList, columns=[\"epitope\",\n",
        "                                                        \"alleles\",\n",
        "                                                        \"total alleles\",\n",
        "                                                        \"antigen\",\n",
        "                                                        ]\n",
        "                            )\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def overlapEpitopes(df, proteins_file):\n",
        "\n",
        "    antigen_dict = {}\n",
        "    alleles_dict = {}\n",
        "    antigen_names_dict = {}\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        antigen = df[\"antigen\"].iloc[i].split(\"|\")\n",
        "        epitope = df[\"epitope\"].iloc[i]\n",
        "        alleles = [x for x in df[\"alleles\"].iloc[i].split() if not \"%\" in x]\n",
        "        \n",
        "        if len(antigen) == 1:\n",
        "            if antigen[0] in antigen_dict:\n",
        "                antigen_dict[antigen[0]].append(epitope)\n",
        "            else:\n",
        "                antigen_dict[antigen[0]] = [epitope]\n",
        "        else:\n",
        "            if antigen in antigen_dict:\n",
        "                antigen_dict[antigen].append(epitope)\n",
        "            else:\n",
        "                antigen_dict[antigen] = [epitope]\n",
        "        \n",
        "        alleles_dict[epitope] = alleles\n",
        "\n",
        "    #We need the original sequences of each protein to map the epitopes\n",
        "    allProteins = {}\n",
        "    with open(proteins_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                proteinFullID=line.strip()[1:]\n",
        "                allProteins[proteinFullID] = \"\"\n",
        "            else:\n",
        "                allProteins[proteinFullID] = allProteins[proteinFullID] + line.strip()\n",
        "\n",
        "    #Map each epitope to the reference\n",
        "    final_tupled_list = []\n",
        "    for origin, epitopes in antigen_dict.items():\n",
        "        \n",
        "        # In the case of multiple origins\n",
        "        if isinstance(origin, list):\n",
        "            print(f\"Epitopes {', '.join(epitopes)} could not be merged.\")\n",
        "            for epitope in epitopes:\n",
        "                newTuple = (epitope, #epitope seq\n",
        "                            origin, #protein of origin\n",
        "                            \", \\n\".join(alleles_dict[epitope]), #alleles binding\n",
        "                            str(len(alleles_dict[epitope])) #total alleles\n",
        "                            )\n",
        "                final_tupled_list.append(newTuple)\n",
        "\n",
        "        # In the case of single origin\n",
        "        else:\n",
        "            refseq = allProteins[origin]\n",
        "\n",
        "            #We map each epitope to the origin sequence to know the order\n",
        "            epitope_order = {}\n",
        "            for epitope in epitopes:\n",
        "                start_aa = refseq.find(epitope)\n",
        "                if start_aa == -1: #not found\n",
        "                    raise Exception (\"Epitope not found\")\n",
        "                epitope_order[start_aa] = epitope\n",
        "\n",
        "            # We will go epitope by epitope and find if they overlap.\n",
        "            # Any overlapping epitopes will be put into the same group\n",
        "            set_of_epitopes = []\n",
        "            current_set = {}\n",
        "            current_pos = None\n",
        "            current_endpos = None\n",
        "            for start_aa, epitope in sorted(epitope_order.items()):  \n",
        "                if current_pos == None: #First epitope\n",
        "                    current_pos = start_aa\n",
        "                    current_endpos = start_aa + len(epitope) - 1\n",
        "                    current_set[start_aa] = epitope\n",
        "\n",
        "                elif current_pos + 1 == start_aa: #Next consecutive epitope\n",
        "                    current_pos = start_aa\n",
        "                    current_endpos = start_aa + len(epitope) - 1\n",
        "                    current_set[start_aa] = epitope\n",
        "                \n",
        "                elif current_pos + 1 < start_aa: #next epitope start is not consecutive...\n",
        "\n",
        "                    if start_aa <= current_endpos: #...but it's still overlapping\n",
        "                        current_pos = start_aa\n",
        "                        current_endpos = start_aa + len(epitope) -1\n",
        "                        current_set[start_aa] = epitope\n",
        "\n",
        "                    else: #...and it is not overlapping with the previous one\n",
        "                        set_of_epitopes.append(current_set) #close this group\n",
        "                        current_set = {} #and empty for next group\n",
        "                        current_pos = start_aa\n",
        "                        current_endpos = start_aa + len(epitope) - 1\n",
        "                        current_set[start_aa] = epitope\n",
        "            set_of_epitopes.append(current_set)\n",
        "\n",
        "            # Now we will create a fused epitope for each group.\n",
        "            # We keep all the alleles that the individual epitopes could bind to\n",
        "            fused_list = []\n",
        "            current_fused = \"\"\n",
        "            current_allele_list = []\n",
        "            for epitope_group in set_of_epitopes:\n",
        "                current_position = None\n",
        "                for position, epitope in epitope_group.items():\n",
        "                    if current_fused == \"\":\n",
        "                        current_fused = epitope\n",
        "                        current_position = position\n",
        "                    else:\n",
        "                        current_fused += epitope[current_position - position :]\n",
        "                        current_position = position\n",
        "                    \n",
        "                    current_allele_list.extend(alleles_dict[epitope])\n",
        "                unique_allele_list = list(set(current_allele_list))\n",
        "                fused_list.append((current_fused, sorted(unique_allele_list)))\n",
        "                current_fused = \"\"\n",
        "                current_allele_list = []\n",
        "            \n",
        "            for fused_epitope in fused_list:\n",
        "                newTuple = (fused_epitope[0], #epitope seq\n",
        "                            origin, #protein of origin\n",
        "                            \", \\n\".join(fused_epitope[1]), #alleles binding\n",
        "                            len(fused_epitope[1]), #total alleles\n",
        "                            )\n",
        "                final_tupled_list.append(newTuple)\n",
        "    \n",
        "    final_df = pd.DataFrame(final_tupled_list, columns=[\"epitope\", \"protein\", \"MHCII_alleles\", \"MCHII_total_alleles\"])\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def fetchUniprotName(protein):\n",
        "    uniprotID = protein.split(\"_\")[1]\n",
        "    url = 'https://www.uniprot.org/uniprot/' + uniprotID\n",
        "    path_name = '//*[@id=\"content-protein\"]/h1'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        byte_data = response.content\n",
        "        source_code = html.fromstring(byte_data)\n",
        "        tree = source_code.xpath(path_name)\n",
        "\n",
        "        if not tree:\n",
        "            return [protein, \"NA\"]\n",
        "        else:\n",
        "            return [protein, str(tree[0].text_content())]\n",
        "    except:\n",
        "        return [protein, \"NA\"]\n",
        "\n",
        "def Uniproter(protlist):    \n",
        "    unique_prots = list(set(protlist))\n",
        "    uniprotDict = {}\n",
        "    results = ThreadPool(20).imap_unordered(fetchUniprotName, unique_prots)\n",
        "    for r in results:\n",
        "        uniprotDict[r[0]] = r[1]\n",
        "    return uniprotDict\n",
        "\n",
        "def processMHC_II(csv_file, MHC_II_cutoff, proteins_fasta=\"./DATA/RAW/allproteins.fa\"):\n",
        "    df = pd.read_csv(csv_file, index_col=0)\n",
        "    df = df[df.MHCII_rank <= MHC_II_cutoff]\n",
        "\n",
        "    df = joinAlleles_MHC_II(df)\n",
        "\n",
        "    df = overlapEpitopes(df, proteins_fasta)\n",
        "\n",
        "    df[\"protein name\"] = df[\"protein\"].map(Uniproter(df[\"protein\"]))\n",
        "\n",
        "    fileName = os.path.basename(csv_file).replace(\"_peptides15.csv\", \".xlsx\")\n",
        "    savePath = \"./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD4\"\n",
        "    saveFile = os.path.join(savePath, fileName)\n",
        "\n",
        "    df.to_excel(saveFile, sheet_name=f\"MHC II <={str(MHC_II_cutoff)}\")\n",
        "\n",
        "def formatMHC_II(csv_dir):\n",
        "    csv_list = [os.path.join(csv_dir, x) for x in os.listdir(csv_dir) if x.endswith(\"peptides15.csv\")]\n",
        "    \n",
        "    for csv_file in csv_list:\n",
        "        processMHC_II(csv_file, \n",
        "                      MHC_II_cutoff = 0.1\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bexCGpIaAz_0"
      },
      "source": [
        "formatMHC_II(\"./DATA/PROCESSED/T_PREDICTIONS/EPITOPES/CD4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2v5InXAJJtr"
      },
      "source": [
        "---\n",
        "# **3. Prediction of B-Cell Epitopes**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0oQVWCrVEFy"
      },
      "source": [
        "## **3a. Prediction of exposed proteins**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiSh8gSs5Ulh"
      },
      "source": [
        "#### Importing list of exposed proteins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCsoqdDb9qpy"
      },
      "source": [
        "from google.colab import files\n",
        "%cd -q \"./DATA/RAW\"\n",
        "uploaded = files.upload()\n",
        "for k, v in uploaded.items():\n",
        "    with open(k, \"r\") as f:\n",
        "        exposedList=[]\n",
        "        for line in f:\n",
        "            exposedList.append(line.strip())\n",
        "%cd -q -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4dEmMZL5lWC"
      },
      "source": [
        "with open(\"./DATA/RAW/allproteins_EXP.fa\",\"a+\") as output:\n",
        "    with open(\"./DATA/RAW/allproteins.fa\", \"r\") as input:\n",
        "        proteins = SeqIO.parse(input, \"fasta\")\n",
        "        for protein in proteins:\n",
        "            if protein.id in exposedList:\n",
        "                output.write(f\">E_{protein.id}\\n\")\n",
        "                output.write(f\"{protein.seq}\\n\")\n",
        "            else:\n",
        "                output.write(f\">{protein.id}\\n\")\n",
        "                output.write(f\"{protein.seq}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2iWoV5bJJuX"
      },
      "source": [
        "## **3b. Protein clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yDL9zTNJJuY"
      },
      "source": [
        "### **CD-HIT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id0p0irVf7Q7"
      },
      "source": [
        "We run the program *CD-HIT* to create protein clusters.\n",
        "\n",
        "Options:\n",
        "\n",
        "```\n",
        "-i --> Input file\n",
        "-o --> Output\n",
        "-d 0 --> Full description of cluster name until first space\n",
        "-c --> % identity cutoff threshold (set to 0.8, deafult is 0.9)\n",
        "-s --> % of length for smaller sequences, we set to 0.75 to avoid strange alignments of short proteins (default 0)\n",
        "-n --> word size\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfx1HNudf7Q8"
      },
      "source": [
        "The input file must be a FASTA file with all the protein sequences to cluster.\n",
        "\n",
        "For this pipeline, each entry has to be in the format `>PROTEOME-ID_PROTEIN-ID`.\n",
        "\n",
        "e.g.:\n",
        "\n",
        "```\n",
        ">UP000000861_P0C9M9\n",
        "MNSLQVLTKKVLIENKAFSNYHEDDSFILQQLGLWWENGPIGFCKQCKMVTGGSMLCSDVDSYELDRALVKAVKENQTDL\n",
        ">UP000000861_P0C9Q0\n",
        "MLPSLQSLTKKVLAGQCLPEDQHYLLKCYDLWWNNAPITFDHNLRLIKSAGLQEGLDLNMALVKAVKENNYSLIKLFTEW\n",
        ">UP000000861_P0C9K9\n",
        "MITLYEAAIKTLITHRKQILKHPDSREILLALGLYWNKTHILLKCHECGKISLTGKHSTKCININCLLILAIKKKNKRMV\n",
        ">UP000000861_P0C8F5\n",
        "MKMHIARDSIVYLLNKHLQNTILTNKIEQECFLQADTPKKYLQYIKPFLINCMTKNITTDLVMKDSKRLEPYITLEMRDI\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br6G0jegf7Q8"
      },
      "source": [
        "#We specify an identity cutoff at 80%, and a minimum length for smaller sequences of 75%\n",
        "\n",
        "!cd-hit -i ./DATA/RAW/allproteins_EXP.fa -o \"./DATA/PROCESSED/B_PREDICTIONS/globalclusters\" -d 0 -c 0.8 -s 0.75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoyjuuT2gybj"
      },
      "source": [
        "### **Cluster filtering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNC-OBvGgybn"
      },
      "source": [
        "We will filter our clusters according to:\n",
        "\n",
        "1.   **Presence of an exposed protein**\n",
        "\n",
        "\n",
        "2.   **Number of proteomes in each cluster**\n",
        "\n",
        "    We want a minimum representation of all strains in our clusters.\n",
        "\n",
        "\n",
        "3.   **Presence of a particular proteome in each cluster**\n",
        "\n",
        "    In case we want a particular proteome to be present always.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8S9fE-egybo"
      },
      "source": [
        "def parseClusters(clstrfile):\n",
        "    returndict = {}\n",
        "    print(f\"Parsing clusters for file {os.path.basename(clstrfile)}:\")\n",
        "    with open(clstrfile, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                clusterID = line.strip()\n",
        "                returndict[clusterID] = []\n",
        "            else:\n",
        "                protein_ID = line.strip().split()[2]\n",
        "                protein_ID = protein_ID[1:protein_ID.find(\"...\")]\n",
        "                returndict[clusterID].append(protein_ID)\n",
        "    print(f\"Total number of clusters: {str(len(returndict))}\")\n",
        "    return returndict\n",
        "\n",
        "def filterExposedClusters(clstrfile,\n",
        "                   min_proteomes, #Minimum number of proteomes in each cluster\n",
        "                   mustProteome=\"\"\n",
        "                   ):\n",
        "    \n",
        "    clusterDict = parseClusters(clstrfile)\n",
        "  \n",
        "    for clusterID, proteins_list in list(clusterDict.items()): #make it a list so we can delete keys\n",
        "        proteome_list = []\n",
        "        exposedFlag = False\n",
        "        for protein in proteins_list:\n",
        "\n",
        "            if protein.startswith(\"E_\"):\n",
        "                exposedFlag = True\n",
        "\n",
        "            proteome = protein[protein.find(\"UP\"):protein.find(\"_\", 2)]\n",
        "            proteome_list.append(proteome)\n",
        "\n",
        "        if not exposedFlag:\n",
        "            del clusterDict[clusterID]\n",
        "\n",
        "        unique_proteomes = list(set(proteome_list))\n",
        "        if len(unique_proteomes) < min_proteomes:\n",
        "            try:\n",
        "                del clusterDict[clusterID]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if mustHaveProteome:\n",
        "            if not mustHaveProteome in unique_proteomes:\n",
        "                try:\n",
        "                    del clusterDict[clusterID]\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    print(f\"Filtered number of clusters: {str(len(clusterDict))}\")\n",
        "\n",
        "    return clusterDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXd3Ni9Mgybq"
      },
      "source": [
        "#In case we want a particular proteome to be present in all clusters, we can\n",
        "#specify it here. Otherwise, leave it as an empty string.\n",
        "\n",
        "mustHaveProteome = \"UP000141072\" #Georgia 2007/1\n",
        "\n",
        "global_clusters = filterExposedClusters(\"./DATA/PROCESSED/B_PREDICTIONS/globalclusters.clstr\",\n",
        "                                 min_proteomes=14,\n",
        "                                 mustProteome=mustHaveProteome\n",
        "                                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPgwMhD7JJui"
      },
      "source": [
        "### **Reconstructing FASTAS**\n",
        "\n",
        "We will create a new FASTA file for each cluster, recreating the sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMV9KkwvlFN0"
      },
      "source": [
        "#In this case, we will give priority to proteomes from genotype II, the current\n",
        "#responsible of ASFV epidemic. We will give two tiers of importance: first \n",
        "#Georgia, and then the rest of genotype II.\n",
        "\n",
        "priorityProteomesList = [\"UP000141072\", #Georgia 2007/1\n",
        "                    \"UP000345145\", #Georgia 2008/2\n",
        "                    \"UP000267045\", #Estonia 2014\n",
        "                    \"UP000326051\", #Lithuania 2014 \n",
        "                    \"UP000325567\", #Moldova 2017\n",
        "                    \"UP000327056\", #CzechRepublic 2017/1\n",
        "                    \"UP000290386\", #China/2018/AnhuiXCGQ\n",
        "                    \"UP000291821\", #China Pig/HLJ/2018\n",
        "                    \"UP000292678\", #China DB/LN/2018\n",
        "                    \"UP000307568\", #Belgium 2018/1\n",
        "                    \"UP000316600\", #China ASFV-wbBS01\n",
        "                    \"UP000324915\", #Belgium Etalle 2018\n",
        "                    \"UP000428265\", #Hungary ASFV_HU_2018\n",
        "                    \"UP000422299\", #China/CAS19-01/2019 \n",
        "                    ]\n",
        "\n",
        "priorityProteomesDict = {}\n",
        "for proteome in priorityProteomesList:\n",
        "    if proteome == \"UP000141072\":\n",
        "        priorityProteomesDict[proteome] = \"01\"\n",
        "    else:\n",
        "        priorityProteomesDict[proteome] = \"02\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M3rlz0BlFN3"
      },
      "source": [
        "#We will parse the original file with all proteins in order to create a complete\n",
        "#dictionary that contains all sequences in a format \"proteinID : sequence\"\n",
        "\n",
        "with open(\"./DATA/RAW/allproteins.fa\", \"r\") as f:\n",
        "    allProteins = {record.id:str(record.seq) for record in SeqIO.parse(f, \"fasta\")}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G_sQV7DlFN4"
      },
      "source": [
        "def reconstruct_fasta(clusterDictionary, savePath, proteomePriorityDict, allProteins=allProteins):\n",
        "    clusterName = os.path.split(os.path.dirname(savePath))[-1]\n",
        "\n",
        "    for clusterID, clusterProteins in clusterDictionary.items():\n",
        "        \n",
        "        fileName = f\"{clusterName}_{clusterID[1:]}.fa\"\n",
        "        filePath = os.path.join(savePath, fileName) \n",
        "\n",
        "        with open(filePath, \"w+\") as f:\n",
        "            for protein in clusterProteins:\n",
        "                protein = protein[protein.find(\"UP\"):]\n",
        "                proteome, proteinID = protein.split(\"_\")\n",
        "\n",
        "                #Here we asign each proteome a priority by adding a prefix\n",
        "                if proteome in proteomePriorityDict:\n",
        "                    f.write(f\">{proteomePriorityDict[proteome]}_{proteome}_{proteinID}\\n\")\n",
        "                else:\n",
        "                    f.write(f\">99_{proteome}_{proteinID}\\n\")\n",
        "                f.write(allProteins[protein]+\"\\n\")\n",
        "\n",
        "    print (f\"FASTAS for {clusterName} reconstructed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTzj6RfqlFN6"
      },
      "source": [
        "globalPath = createPath(\"./DATA/PROCESSED/B_PREDICTIONS/FASTAS CLUSTERS/GLOBAL/\")\n",
        "reconstruct_fasta(global_clusters, globalPath, priorityProteomesDict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scG3OtBNxnyL"
      },
      "source": [
        "## **3c. Multiple sequence alignment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwX7x-PqxnyN"
      },
      "source": [
        "### **Running MUSCLE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW3FPZ3VxnyN"
      },
      "source": [
        "We run the MUSCLE MSA tool for each cluster. It results in a new MSA fasta file stored in MUSCLE ALIGNMENTS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1llvQ-uUxnyO"
      },
      "source": [
        "def runMUSCLE(infile, outfile):\n",
        "    #Here is where we installed MUSCLE\n",
        "    muscle_exe = r\"/usr/local/bin/muscle3.8.31_i86linux64\"\n",
        "\n",
        "    muscle_cline = MuscleCommandline(muscle_exe,\n",
        "                                     input=infile,\n",
        "                                     out=outfile,\n",
        "                                     clwstrict=True\n",
        "                                    )\n",
        "    muscle_cline()\n",
        "\n",
        "def alignDir(fastas_dir):\n",
        "    dirname = os.path.basename(os.path.normpath(fastas_dir))\n",
        "\n",
        "    print (\"Aligning \" + dirname + \" clusters\")\n",
        "    print (\"-----------------------------------\")\n",
        "\n",
        "    out_path = createPath(\"./DATA/PROCESSED/B_PREDICTIONS/MUSCLE ALIGNMENTS/UNSORTED/\" + dirname)\n",
        "\n",
        "    for root, dirs, files in os.walk(fastas_dir):\n",
        "        for fasta in tqdm(files):\n",
        "            if fasta.endswith(\".fa\"):\n",
        "                in_file = os.path.join(root, fasta)\n",
        "                out_file = out_path + \"/MSA_\" + fasta\n",
        "                runMUSCLE(in_file, out_file)\n",
        "    print(\"-------------------------------------\")    \n",
        "    print (\"All MSAs for \"+ str(dirname) +\" done!\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td_7jjiMxnyQ"
      },
      "source": [
        "for root, dirs, files in os.walk(\"./DATA/PROCESSED/B_PREDICTIONS/FASTAS CLUSTERS/\"):\n",
        "    for dir in dirs:\n",
        "        if not dir.startswith(\".\"):\n",
        "            alignDir(os.path.join(root, dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7Bg17UxnyS"
      },
      "source": [
        "### **Sorting the MSAs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHcCXIFFxnyT"
      },
      "source": [
        "We will now open each MSA and sort it first according to our proteome priority, and then by number of positions aligned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGivku1zxnyT"
      },
      "source": [
        "def keyMSA(item):\n",
        "    #We need a custom function to sort the MSA by priority and length of alignment\n",
        "    returnList = []\n",
        "    returnList.append(1 - int(item.id.split(\"_\")[0])) #(1 - x) because descending order\n",
        "    returnList.append(len(item) - item.seq.count(\"-\"))\n",
        "    return tuple(returnList)\n",
        "\n",
        "def sortMSA(msafile):\n",
        "    alignment = AlignIO.read(msafile,\"clustal\")\n",
        "    alignment.sort(key = keyMSA, reverse=True)\n",
        "    return alignment\n",
        "\n",
        "def sortMSA_dir(in_folder):\n",
        "    musclefiles = [x for x in os.listdir(in_folder) if x.endswith(\".fa\")]\n",
        "    clusterName = os.path.basename(os.path.normpath(in_folder))\n",
        "    dir_sorted = createPath(f\"./DATA/PROCESSED/B_PREDICTIONS/MUSCLE ALIGNMENTS/SORTED/{clusterName}/\")\n",
        "\n",
        "    for m in musclefiles:\n",
        "        infile = os.path.join(in_folder,m)\n",
        "        aln = sortMSA(infile)\n",
        "        with open(os.path.join(dir_sorted, m[:-3] + \"_sorted.aln\"),\"w+\") as outfile:\n",
        "            AlignIO.write(aln, outfile, \"clustal\")\n",
        "\n",
        "    print(f\"Sorted {str(len(musclefiles))} {clusterName} alignments\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2PWbKy5xnyV"
      },
      "source": [
        "for root, dirs, files in os.walk(\"./DATA/PROCESSED/B_PREDICTIONS/MUSCLE ALIGNMENTS/UNSORTED\"):\n",
        "    for dir in dirs:\n",
        "        if not dir.startswith(\".\"):\n",
        "            msa_dir = os.path.join(root,dir)\n",
        "            sortMSA_dir(msa_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqtTZ2LjyjC2"
      },
      "source": [
        "## **3d. Entropy and consensus sequence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ipK9sLbyjC3"
      },
      "source": [
        "### **Calculate entropy and generate consensus sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EV7EoY3yjC4"
      },
      "source": [
        "We will use the Shannon entropy to calculate the variability in each position of each MSA. If a position has an entropy above the selected threshold (by default 0), it will be masked with an asterisk(*), thus generating a masked consensus sequence.\n",
        "\n",
        "Then, we will find the sequence from the MSA that most closely resembles the masked consensus, and mask the same positions as the masked consensus. This way, we end up with a \"real\" masked sequence, not a consensus \"artificial\" one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWcUokp1yjC4"
      },
      "source": [
        "def shannonEntropy(data):\n",
        "    data_series = pd.Series(data)\n",
        "    counts = data_series.value_counts()\n",
        "    shannon_entropy = scipy.stats.entropy(counts)\n",
        "    return shannon_entropy\n",
        "\n",
        "def Shannon(alnfile, shannon_threshold=0):   \n",
        "    clusterName = os.path.dirname(alnfile).split(\"/\")[-1]\n",
        "    shannon_str = str(shannon_threshold).replace(\".\", \"\")\n",
        "    dest_pref = f\"./DATA/PROCESSED/B_PREDICTIONS/CONSENSUS/SHANNON_{shannon_str}\"\n",
        "    dest_dir = createPath(f\"{dest_pref}/{clusterName}/\")\n",
        "    dest_file = os.path.basename(alnfile).replace(\"sorted.aln\",\n",
        "                                                  f\"consensus{shannon_str}.fasta\"\n",
        "                                                )\n",
        "    \n",
        "    #It will go position by position, calculate the entropy and then create a\n",
        "    #consensus sequence with the most common amino acid, unless the entropy is \n",
        "    #higher than the threshold, in which case it will insert an *.\n",
        "    consensus_list = []\n",
        "    alignment = AlignIO.read(alnfile, \"clustal\")\n",
        "    for i in range(0,alignment.get_alignment_length()):\n",
        "        aa = []\n",
        "        for record in alignment:\n",
        "            aa.append(record.seq[i])\n",
        "        position_entropy = shannonEntropy(aa)\n",
        "        if position_entropy <= shannon_threshold:\n",
        "            consensus_list.append(Counter(aa).most_common()[0][0])\n",
        "        else:\n",
        "            consensus_list.append(\"*\")\n",
        "    consensus_seq = \"\".join(consensus_list)\n",
        "\n",
        "    #To find the closest sequence to the consensus, we will do a pairwise alignment\n",
        "    #between the consensus and all sequences from the MSA. The sequence with highest\n",
        "    #score will be the closest one. In case of equal score, prioritized proteomes\n",
        "    #have advantage.\n",
        "    refSeq = None\n",
        "    highestScore = 0\n",
        "    for record in alignment:\n",
        "        pairwise_score = pairwise2.align.globalxx(record.seq,\n",
        "                                                consensus_seq,\n",
        "                                                score_only=True\n",
        "                                                )\n",
        "        if pairwise_score > highestScore:\n",
        "            highestScore = pairwise_score\n",
        "            refSeq = record\n",
        "        elif pairwise_score == highestScore:\n",
        "            if int(record.id[0:2]) < int(refSeq.id[0:2]):\n",
        "                refSeq = record\n",
        "    \n",
        "    #Finally, we will mask the positions of the closest sequence according to\n",
        "    #the masked consensus sequence.\n",
        "    refSeq_aa = list(refSeq.seq)\n",
        "    masked_ref = [\"*\"] * len(refSeq_aa)\n",
        "    for i in range(0, len(refSeq_aa)):\n",
        "        if consensus_seq[i] != \"*\":\n",
        "            masked_ref[i] = refSeq_aa[i]\n",
        "    \n",
        "    #We write the results in a new fasta file\n",
        "    with open(dest_dir + dest_file, \"w+\") as f:\n",
        "        f.write(f\">{refSeq.id}\\n\")\n",
        "        f.write(\"\".join(masked_ref))\n",
        "\n",
        "    #All results from the same cluster type and Shannon threshold will be appended\n",
        "    #in a file.\n",
        "    mergedFile = (f\"{dest_pref}/{clusterName}_fullconsensus{shannon_str}.fasta\")\n",
        "    with open(mergedFile, \"a+\") as f:\n",
        "        f.write(f\">{refSeq.id}\\n\")\n",
        "        f.write(\"\".join(masked_ref)+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmJHm53oyjC6"
      },
      "source": [
        "#In case we want to calculate the consensus for different thresholds, we can\n",
        "#put them in a list here:\n",
        "shannon_thresholds = [0,\n",
        "                      #0.25,\n",
        "                      #0.5\n",
        "                      ]\n",
        "\n",
        "for s in shannon_thresholds:\n",
        "    for dir in os.listdir(\"./DATA/PROCESSED/B_PREDICTIONS/MUSCLE ALIGNMENTS/SORTED\"):\n",
        "        if not dir.startswith(\".\"):\n",
        "            cluster_dir = os.path.join(\"./DATA/PROCESSED/B_PREDICTIONS/MUSCLE ALIGNMENTS/SORTED\", dir)\n",
        "            print(f\"Calculating invariable proteome at Shannon {s} for {dir}\")\n",
        "            for f in tqdm(os.listdir(cluster_dir)):\n",
        "                if f.endswith(\".aln\"):\n",
        "                    Shannon(os.path.join(cluster_dir, f), s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSJilIBXzbFh"
      },
      "source": [
        "## **3e. Validation of IEDB B-cell epitopes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O534rH3pzbFl"
      },
      "source": [
        "### **Downloading IEDB epitopes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMATzs_HzbFl"
      },
      "source": [
        "We will download the available confirmed epitopes of our organism from IEDB via Selenium."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_xpo0brzbFl"
      },
      "source": [
        "#We need the ID of our organism at IEDB. For example, for ASFV is 10497, and for\n",
        "#T. cruzi is 5693. It can be found at IEDB.org and searching in \"Organism\".\n",
        "\n",
        "IEDB_ID = 10497"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHJyvv4CzbFo"
      },
      "source": [
        "%cd -q \"./DATA/RAW\"\n",
        "\n",
        "wd = webdriver.Chrome('chromedriver', options=options)\n",
        "wd.get(f\"http://www.iedb.org/sourceOrgId/{IEDB_ID}\")\n",
        "\n",
        "wd.find_element_by_xpath('//*[@id=\"content\"]/div/div[2]/table/tbody/tr[4]/td[2]/div/a').click() #Click on B assays link\n",
        "time.sleep(5)\n",
        "\n",
        "wd.find_element_by_css_selector('#refine_container > div:nth-child(8) > div.search_content > div.ui_input.check.search_bothlinks > div.checkbox.square').click() #Positive assays only\n",
        "time.sleep(2)\n",
        "\n",
        "wd.find_element_by_css_selector('#refine_container > button:nth-child(4)').click() #Search\n",
        "time.sleep(2)\n",
        "\n",
        "wd.find_element_by_css_selector('#result_carousel > div.carousel_content.active > div > div.content_tabholder > div:nth-child(2) > div.content_tab > div.content_text').click() #B cell tab\n",
        "time.sleep(2)\n",
        "\n",
        "wd.find_element_by_css_selector('#result_carousel > div.carousel_content.active > div > div.contenttable_content.active > div > div:nth-child(1) > div.exportholder').click() #Export results\n",
        "time.sleep(2)\n",
        "\n",
        "wd.find_element_by_css_selector(\"div.inwindow_popup.drop_arrow.identifier_container.show > div > div:nth-child(1) > div.exportholder > div.txt\").click() #Popup Export to CSV file\n",
        "time.sleep(2)\n",
        "\n",
        "wd.switch_to.window(wd.window_handles[-1]) #Switch to \"download\" page\n",
        "time.sleep(2)\n",
        "\n",
        "wd.refresh() #We refresh to prompt the download\n",
        "time.sleep(2)\n",
        "\n",
        "wd.close()\n",
        "\n",
        "%cd -q ../../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvt2rDeqzbFr"
      },
      "source": [
        "for f in os.listdir(\"./DATA/RAW/\"):\n",
        "    if f.endswith(\".zip\"):\n",
        "        if f.startswith(\"bcell\"):\n",
        "            zipname = \"./DATA/RAW/\" + f\n",
        "            !unzip $zipname -d \"./DATA/RAW/\"\n",
        "            os.remove(zipname)\n",
        "\n",
        "for f in os.listdir(\"./DATA/RAW/\"):\n",
        "    if f.endswith(\".csv\"):\n",
        "        if f.startswith(\"bcell\"):\n",
        "            os.rename(r\"./DATA/RAW/\" + f, r\"./DATA/RAW/B_Cell_IEDB_epitopes.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwOnSFJczbFt"
      },
      "source": [
        "IEDB_B_epitopes = {}\n",
        "\n",
        "with open(\"./DATA/RAW/B_Cell_IEDB_epitopes.csv\",\"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    next(reader)\n",
        "    for line in reader:\n",
        "        IEDB_B_epitopes[line[0]] = (line[11], #Sequence\n",
        "                                    line[15], #Antigen\n",
        "                                    line[17], #Antigen ID\n",
        "                                    line[71], #Assay 1\n",
        "                                    line[72], #Assay 2\n",
        "                                    line[74]  #Assay 3\n",
        "                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnTjL9gVzbFu"
      },
      "source": [
        "### **Identify conserved IEDB epitopes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7ygsd_czbFu"
      },
      "source": [
        "def IEDB_conserved(consensus_file, epitopes_dict):\n",
        "    conserved_records = list(SeqIO.parse(consensus_file, \"fasta\"))\n",
        "    conservedEpis = []\n",
        "\n",
        "    for record in conserved_records:\n",
        "        sequence = str(record.seq)\n",
        "        for epitope_id, epitope_data in epitopes_dict.items():\n",
        "            if epitope_data[0] in sequence:\n",
        "                epitope_entry = [epitope_id,\n",
        "                                 epitope_data[0],\n",
        "                                 record.id,\n",
        "                                 epitope_data[1],\n",
        "                                 epitope_data[2],\n",
        "                                 \" \".join(epitope_data[3:])]\n",
        "                conservedEpis.append(tuple(epitope_entry))\n",
        "\n",
        "    df = pd.DataFrame(conservedEpis,\n",
        "                      columns=['Epitope_ID',\n",
        "                               'Epitope',\n",
        "                               'Antigen',\n",
        "                               'IEDB_Antigen name',\n",
        "                               'IEDB_Antigen ID',\n",
        "                               'IEDB_Assay'\n",
        "                               ]\n",
        "                      )\n",
        "\n",
        "    clusterN = os.path.basename(consensus_file).split(\"_\")[0]\n",
        "    shannonN = os.path.dirname(consensus_file).split(\"/\")[-1].split(\"_\")[-1]\n",
        "\n",
        "    output_path = createPath(consensus_file.split(\"CONSENSUS\")[0] + \"IEDB/\")\n",
        "    output_path = f\"{output_path}{clusterN}_{shannonN}_IEDB_epis.csv\"\n",
        "\n",
        "    df.to_csv(output_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5nqGCgPzbFw"
      },
      "source": [
        "consensusFolder = \"./DATA/PROCESSED/B_PREDICTIONS/CONSENSUS/\"\n",
        "\n",
        "for shannon in os.listdir(consensusFolder):\n",
        "    for f in os.listdir(consensusFolder + shannon):\n",
        "        if f.endswith(\".fasta\"):\n",
        "            IEDB_conserved(os.path.join(consensusFolder + shannon, f), IEDB_B_epitopes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrKo3FQ5NKO"
      },
      "source": [
        "## **3f. *De novo* structure-based epitope predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxXecGjkdu3y"
      },
      "source": [
        "### **Finding and downloading PDB structures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbiMegnkJApE"
      },
      "source": [
        "#### PDB BLAST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIVB0raMJElT"
      },
      "source": [
        "with open(createPath(\"./DATA/PROCESSED/B_PREDICTIONS/STRUCTURE_BASED/\") + \"ExposedSequences.fasta\", \"w+\") as output:\n",
        "    with open(\"./DATA/RAW/allproteins.fa\", \"r\") as input:\n",
        "        allProteins = {record.id:str(record.seq) for record in SeqIO.parse(input, \"fasta\")}\n",
        "        for protein in exposedList:\n",
        "            output.write(f\">{protein}\\n\")\n",
        "            output.write(allProteins[protein] + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbTm6SlzOzFS"
      },
      "source": [
        "exposedFasta = open('./DATA/PROCESSED/B_PREDICTIONS/STRUCTURE_BASED/ExposedSequences.fasta')\n",
        "\n",
        "result_handle = NCBIWWW.qblast(\"blastp\",\n",
        "                               \"pdb\",\n",
        "                               exposedFasta.read(),\n",
        "                               )\n",
        "\n",
        "with open('./DATA/PROCESSED/B_PREDICTIONS/STRUCTURE_BASED/PDB_BLAST_results.xml', 'w+') as save_file: \n",
        "    blast_results = result_handle.read() \n",
        "    save_file.write(blast_results)\n",
        "\n",
        "exposedFasta.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr76oRU6PurM"
      },
      "source": [
        "def PDBblast_parser(xml, min_identity=0.9):\n",
        "    handle = open(xml)\n",
        "    records = NCBIXML.parse(handle)\n",
        "    \n",
        "    best_hits = {}\n",
        "    \n",
        "    for record in records:\n",
        "        epitope_length = record.query_length\n",
        "        \n",
        "        hits = []\n",
        "        \n",
        "        if record.alignments:\n",
        "            for alignment in record.alignments:\n",
        "                for hsp in alignment.hsps:\n",
        "                    if float(hsp.identities) / int(epitope_length) >= min_identity: #we want a minimum identity\n",
        "                        hits.append((alignment.title.split(\"|\")[1],\n",
        "                                    float(hsp.identities) / int(epitope_length)\n",
        "                                    *100,\n",
        "                                    hsp.expect,\n",
        "                                    ))\n",
        "            if hits:                    \n",
        "                best_hits[record.query] = [x for x in hits]\n",
        "            \n",
        "    handle.close()\n",
        "                \n",
        "    return best_hits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5n5h7BjndJr"
      },
      "source": [
        "#### Download .pdb files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftZH0KarRxyJ"
      },
      "source": [
        "for key, values in PDBblast_parser(\"./DATA/PROCESSED/B_PREDICTIONS/STRUCTURE_BASED/PDB_BLAST_results.xml\").items():\n",
        "    for value in values:\n",
        "        PDB_id = value[0]\n",
        "        url = f\"https://files.rcsb.org/download/{PDB_id}.pdb\"\n",
        "        !wget $url\n",
        "\n",
        "listPDBfiles = [x for x in os.listdir() if x.endswith(\".pdb\")]\n",
        "pdb_dir = createPath(\"./DATA/PROCESSED/B_PREDICTIONS/STRUCTURE_BASED/PDBs\")\n",
        "for pdb in listPDBfiles:\n",
        "    !mv $pdb '$pdb_dir'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AJTsvxAd5Eq"
      },
      "source": [
        "### **PDB structures analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwvirrqHuUJX"
      },
      "source": [
        "#### Installing NACCESS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQMGzHgV60tL"
      },
      "source": [
        "#Upload the already decrypted tar.gz file\n",
        "!mkdir -p ./TOOLS/NACCESS/\n",
        "from google.colab import files\n",
        "naccess_pack = files.upload()\n",
        "naccess_filename = next(iter(naccess_pack))\n",
        "!tar -xzf {naccess_filename} -C ./TOOLS/NACCESS\n",
        "!rm {naccess_filename}\n",
        "\n",
        "for dir in os.listdir(\"./TOOLS/NACCESS/\"):\n",
        "    if not dir.startswith(\".\"):\n",
        "        naccessDir = f\"./TOOLS/NACCESS/{dir}\"\n",
        "\n",
        "#Modify the NACCESS script to patch a common error\n",
        "accall_lines = []\n",
        "with open(os.path.join(naccessDir, \"accall.f\"), \"r\") as f:\n",
        "    for line in f:\n",
        "        accall_lines.append(line.strip(\"\\n\"))\n",
        "correctedLine = \"                  write(4,*)\"\n",
        "accall_lines[254] = correctedLine\n",
        "\n",
        "with open(os.path.join(naccessDir, \"accall.f\"), \"w+\") as f:\n",
        "    for line in accall_lines:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "#Finally, install the program\n",
        "%cd -q $naccessDir\n",
        "!chmod +x install.scr\n",
        "!chmod +x naccess.scr\n",
        "!csh install.scr\n",
        "%cd -q -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrxaO10suXOD"
      },
      "source": [
        "#### Process PDB files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIQLh3zdHAxi"
      },
      "source": [
        "def B_structure_based_epitopes(pdbfile, consensusFile):\n",
        "\n",
        "    PDB_id = os.path.basename(pdbfile).split(\".\")[0]\n",
        "\n",
        "    #Run NACCESS\n",
        "    !./TOOLS/NACCESS/naccess2.1.1/naccess $pdbfile\n",
        "\n",
        "    #Parse the .rsa file generated by NACCESS\n",
        "    RSA_file = f\"{PDB_id}.rsa\"\n",
        "    RSA_values = {}\n",
        "    with open(RSA_file, \"r\") as input:\n",
        "        for line in input:\n",
        "            if line.startswith(\"RES\"):\n",
        "                aa = line[4:7].strip()\n",
        "                chain = line[8:9].strip()\n",
        "                position = int(line[9:13].strip())\n",
        "                rsa = float(line[23:29].strip())\n",
        "                RSA_values[(chain, position)] = (aa, rsa)\n",
        "    \n",
        "    #Move all naccess-generated files\n",
        "    destNaccess = createPath(\"./DATA/PROCESSED/B_PREDICTIONS/STRUCTURE_BASED/NACCESS_results/\")\n",
        "    naccess_files = [x for x in os.listdir() if x.startswith(PDB_id)]\n",
        "    for f in naccess_files:\n",
        "        !mv $f $destNaccess\n",
        "    \n",
        "    #Parse the B values from the PDB file\n",
        "    PDB_values = {}\n",
        "    with open(pdbfile, \"r\") as input_f:\n",
        "        for line in input_f:\n",
        "            if line.startswith(\"ATOM\"):\n",
        "                data = line.strip()\n",
        "                if \"CA\" in data:\n",
        "                    try:\n",
        "                        PDB_values[(data[21:22].strip(), int(data[23:30].strip()))] = (data[17:20], float(data[60:66].strip()))\n",
        "                    except:\n",
        "                        raise Exception(\"Something went wrong with the PDB file\")\n",
        "\n",
        "    #Normalize the B factors. For this we need to calculate first\n",
        "    #the mean of all B values as well as their standard deviation.\n",
        "    B_values = []\n",
        "    for key, value in PDB_values.items():\n",
        "        B_values.append(value[1])     \n",
        "    a = np.array(B_values)\n",
        "    B_mean = a.mean()\n",
        "    B_std = a.std()\n",
        "    \n",
        "    #Change all the PDB raw values for their normalized ones\n",
        "    PDB_normalized = {}\n",
        "    for key, value in PDB_values.items():\n",
        "        normB = (value[1] - B_mean) / B_std\n",
        "        PDB_normalized[key] = (value[0], normB)\n",
        "\n",
        "    #Search for flexible regions according to B-factor calculations\n",
        "    flexible_regions = []\n",
        "    current_region = []\n",
        "\n",
        "    for key, value in sorted(PDB_normalized.items()):\n",
        "        if value[1] >= 1: #We want a minimum of 1 normalized flexibility\n",
        "            current_region.append((key[0], key[1], value[0], value[1]))\n",
        "        else:\n",
        "            if len(current_region) >= 8: #minimum epitope length\n",
        "                flexible_regions.append(current_region)\n",
        "                current_region = []\n",
        "            else:\n",
        "                current_region = []\n",
        "\n",
        "    #Add RSA values to these regions\n",
        "    potential_regions = []\n",
        "\n",
        "    for region in flexible_regions:\n",
        "        potential_region = []\n",
        "\n",
        "        for position in region:\n",
        "            if RSA_values[(position[0], position[1])][0] == position[2]:\n",
        "                valuesTuple = (position[0], position[1], position[2], position[3], RSA_values[(position[0], position[1])][1])\n",
        "                potential_region.append(valuesTuple)\n",
        "            else:\n",
        "                raise Exception(\"Aminoacids do not match\")  \n",
        "\n",
        "        potential_regions.append(potential_region)\n",
        "\n",
        "    #We will change the aminoacid names for their one-letter code\n",
        "    d_aa = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K', \n",
        "        'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',\n",
        "        'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W', \n",
        "        'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
        "\n",
        "    #For each region, we will select all fragments with the required rsa and flex\n",
        "    fragments_list = []\n",
        "\n",
        "    for region in potential_regions:\n",
        "        epis = []\n",
        "        \n",
        "        for position_n, position in enumerate(region):\n",
        "            for i in range(len(region) + 1):\n",
        "                if position_n + i <= len(region):\n",
        "                    \n",
        "                    current_chain = position[0] + \" \" + str(position[1])\n",
        "                    current_epi = region[position_n:i]  \n",
        "                    epi_seq = \"\".join([d_aa[x[2]] for x in current_epi])\n",
        "                    \n",
        "                    if len(epi_seq) >= 8:   \n",
        "                        \n",
        "                        mean_rsa = np.array([x[4] for x in current_epi]).mean()\n",
        "                        mean_flex = np.array([x[3] for x in current_epi]).mean()\n",
        "\n",
        "                        if mean_rsa >= 50: #minimum RSA\n",
        "                            if mean_flex >= 1: #minimum flex\n",
        "                                epis.append((epi_seq, current_chain, mean_flex, mean_rsa))\n",
        "            \n",
        "        fragments_list.append(epis)\n",
        "\n",
        "    #We have to see if these epitopes are conserved in the shannon analysis\n",
        "    hits_dict = defaultdict(list)\n",
        "    \n",
        "    invariable_sequences = [(str(x.seq), str(x.id)) for x in list(SeqIO.parse(consensusFile, \"fasta\"))]\n",
        "\n",
        "    conserved_list = []\n",
        "    for fragments in fragments_list:\n",
        "        conserved_fragments = []\n",
        "        for fragment in fragments:\n",
        "            for sequence in invariable_sequences:\n",
        "                if fragment[0] in sequence[0]:\n",
        "                    hits_dict[fragment[0]].append(sequence[1])\n",
        "                    conserved_fragments.append(fragment)\n",
        "        conserved_list.append(conserved_fragments)\n",
        "\n",
        "\n",
        "    #We keep the largest epitope of each fragment. Ideally, at this point we should\n",
        "    #check all fragments with BLAST first. However, it's more likely that larger\n",
        "    #fragments have a lower identity with any given protein.\n",
        "\n",
        "    B_epis = []\n",
        "    for epi_list in conserved_list:\n",
        "        if epi_list:\n",
        "            sorted_epis = sorted(epi_list, key=lambda x: len(x[0]), reverse=True)\n",
        "            B_epis.append(sorted_epis[0])\n",
        "\n",
        "    #We create a Dataframe with the final epitopes, and add the consensus hits\n",
        "\n",
        "    df = pd.DataFrame(B_epis, columns=[\"epitope\", \"chain\", \"mean flex\", \"mean rsa\"])\n",
        "    df['origin'] = df[\"epitope\"].map(hits_dict)\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49tBGSgoINhX"
      },
      "source": [
        "pdbfiles = [os.path.join(pdb_dir, x) for x in os.listdir(pdb_dir) if x.endswith(\".pdb\")]\n",
        "consensusFolders =[f\"./DATA/PROCESSED/B_PREDICTIONS/CONSENSUS/{x}/\" for x in os.listdir(\"./DATA/PROCESSED/B_PREDICTIONS/CONSENSUS\") if x.startswith(\"SHANNON\")]\n",
        "for consensus_folder in consensusFolders:\n",
        "    consensusFiles = [os.path.join(consensus_folder, x) for x in os.listdir(consensus_folder) if x.endswith(\".fasta\")]\n",
        "    for consensus_file in consensusFiles:\n",
        "        shannon = os.path.normpath(consensus_folder).split(\"/\")[-1] \n",
        "        group = os.path.basename(consensus_file).split(\"_\")[0]\n",
        "\n",
        "        destPath = createPath(\"./DATA/PROCESSED/B_PREDICTIONS/EPITOPES/STRUCTURE_BASED/\")\n",
        "        destFile = destPath + group + \"_\" + shannon\n",
        "        df = pd.DataFrame()\n",
        "\n",
        "        for pdb_f in pdbfiles:\n",
        "            df_pdb = B_structure_based_epitopes(os.path.abspath(pdb_f), consensus_file)\n",
        "            df_pdb[\"PDB_ID\"] = os.path.basename(pdb_f).split(\".\")[0]\n",
        "            df = df.append(df_pdb, ignore_index = True)\n",
        "\n",
        "        df.to_excel(destFile + \".xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PND3ZRjsAg3l"
      },
      "source": [
        "CONTINUE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu-QIfJDvbtv"
      },
      "source": [
        "## **3g. *De novo* sequence-based epitope predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLVnEbqIi5jj"
      },
      "source": [
        "### **Install BepiPred2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jglxD_35ksML"
      },
      "source": [
        "#### Installing prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70aRBHqct8g4"
      },
      "source": [
        "BLAST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6fXocBB8spt"
      },
      "source": [
        "!wget https://ftp.ncbi.nlm.nih.gov/blast/executables/legacy.NOTSUPPORTED/2.2.18/blast-2.2.18-x64-linux.tar.gz\n",
        "!mkdir -p ./TOOLS/BLAST/\n",
        "!tar -xzf blast-2.2.18-x64-linux.tar.gz -C ./TOOLS/BLAST/\n",
        "!rm blast-2.2.18-x64-linux.tar.gz\n",
        "blastpgp_path = os.path.abspath(\"./TOOLS/BLAST/blast-2.2.18/bin/blastpgp\")\n",
        "blast_data_path = os.path.abspath(\"./TOOLS/BLAST/blast-2.2.18/data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ufzja6usLmk"
      },
      "source": [
        "NetSurfP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtjOVtnqbOnU"
      },
      "source": [
        "#Upload the .tar.gz file, uncompress and untar it.\n",
        "!mkdir -p ./TOOLS/NETSURFP/\n",
        "from google.colab import files\n",
        "netsurfp_pack = files.upload()\n",
        "netsurfp_filename = next(iter(netsurfp_pack))\n",
        "!tar -xzf {netsurfp_filename} -C ./TOOLS/NETSURFP\n",
        "!rm {netsurfp_filename}\n",
        "\n",
        "for dir in os.listdir(\"./TOOLS/NETSURFP/\"):\n",
        "    if not dir.startswith(\".\"):\n",
        "        netsurfpDir = os.path.abspath(f\"./TOOLS/NETSURFP/{dir}\")\n",
        "        netsurfpScript = f\"{netsurfpDir}/netsurfp\"\n",
        "        netsurfpTmp = f\"{netsurfpDir}/tmp\"\n",
        "\n",
        "!wget http://www.cbs.dtu.dk/services/NetSurfP-1.0/data.tar.gz\n",
        "!tar -xzf data.tar.gz -C $netsurfpDir\n",
        "!rm data.tar.gz\n",
        "!wget http://www.cbs.dtu.dk/services/NetSurfP-1.0/nr70_db.tar.gz\n",
        "!tar -xzf nr70_db.tar.gz -C $netsurfpDir\n",
        "!rm nr70_db.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3-ynaEUbOnV"
      },
      "source": [
        "#Modify the NetSurfP script settings following the README file instructions\n",
        "parsedNetSurf = []\n",
        "with open(netsurfpScript, \"r\") as f:\n",
        "    for line in f:\n",
        "        parsedNetSurf.append(line.strip(\"\\n\"))\n",
        "\n",
        "for counter, line in enumerate(parsedNetSurf):\n",
        "    if line.startswith(\"  $ENV{NetSurfP}\"):\n",
        "        parsedNetSurf[counter] = \"  $ENV{NetSurfP}\t= '%s';\" % (netsurfpDir)\n",
        "    if line.startswith(\"  $ENV{BLASTPROG}\"):\n",
        "        parsedNetSurf[counter] = \"  $ENV{BLASTPROG}\t= '%s';\" % (blastpgp_path)\n",
        "    if line.startswith(\"  $ENV{BLASTMAT}\"):\n",
        "        parsedNetSurf[counter] = \"  $ENV{BLASTMAT}\t= '%s';\" % (blast_data_path)\n",
        "    if line.startswith(\"  $ENV{TMPDIR}\"):\n",
        "        parsedNetSurf[counter] = \"  $ENV{TMPDIR}\t\t= '%s';\" % (netsurfpTmp)\n",
        "    \n",
        "with open(netsurfpScript, \"w+\") as f:\n",
        "    for line in parsedNetSurf:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "!chmod 755 {netsurfpScript}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETZ-OfVukZXq"
      },
      "source": [
        "Also outdated packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36dItixkm-h9"
      },
      "source": [
        "!pip install scipy==1.2.3 --no-cache-dir\n",
        "!pip install numpy==1.16.6 --no-cache-dir\n",
        "!pip install  matplotlib==2.0.0 --no-cache-dir\n",
        "!pip install scikit-learn==0.17 --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALo_umg0bXtx"
      },
      "source": [
        "#### Downloading and installing the IEDB B-cell tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi_-g39hkrnC"
      },
      "source": [
        "b_url = \"https://downloads.iedb.org/tools/bcell/3.1/IEDB_BCell-3.1.tar.gz\"\n",
        "b_filename = b_url.split(\"/\")[-1]\n",
        "\n",
        "print(\"Downloading IEDB B-cell tool...\")\n",
        "!wget $b_url\n",
        "!mkdir -p ./TOOLS/BCELL/\n",
        "!tar -xzf $b_filename -C ./TOOLS/BCELL/\n",
        "!rm $b_filename\n",
        "\n",
        "for dir in os.listdir(\"./TOOLS/BCELL/\"):\n",
        "    if not dir.startswith(\".\"):\n",
        "        bcell_dir = os.path.abspath(f\"./TOOLS/BCELL/{dir}\")\n",
        "        bcell_script = f\"{bcell_dir}/predict_antibody_epitope.py\"\n",
        "\n",
        "%cd -q $bcell_dir\n",
        "\n",
        "%env NETSURFP_BIN=$netsurfpScript\n",
        "!./configure\n",
        "\n",
        "%cd -q -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hIsYv9Rip_h"
      },
      "source": [
        "### **Run BepiPred2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsS_963Zvbtz"
      },
      "source": [
        "First, we create a new unmasked file with the consensus sequences that we will feed to the prediction algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frZ21ERGS39z"
      },
      "source": [
        "def getConsensus(in_dir):\n",
        "    shannon = in_dir.split(\"_\")[-1]   \n",
        "    for f in os.listdir(in_dir):\n",
        "        if f.endswith(\".fasta\"):\n",
        "            output = f.replace(\".fasta\", \"_unmasked.fasta\")\n",
        "            prots = [str(record.id) for record in SeqIO.parse(os.path.join(in_dir, f), \"fasta\")]\n",
        "            with open(os.path.join(in_dir, output), \"w+\") as output_f:\n",
        "                for protein in prots:\n",
        "                    output_f.write(\">\" + protein + \"\\n\")\n",
        "                    output_f.write(allProteins[protein[3:]] + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0krzk0dYZHI"
      },
      "source": [
        "for subdir in os.listdir(\"./DATA/PROCESSED/B_PREDICTIONS/CONSENSUS\"):\n",
        "    getConsensus(os.path.join(\"./DATA/PROCESSED/B_PREDICTIONS/CONSENSUS\", subdir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2I_Fg-dQOEL"
      },
      "source": [
        "Then, we define a function that will take our unmasked file and pass it through Bepipred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqV3f8wAPqJ9"
      },
      "source": [
        "def Bepipred_prediction(filepath):\n",
        "    starttime = time.time()\n",
        "    rootdir = os.path.dirname(filepath)\n",
        "    basename = os.path.basename(filepath)\n",
        "    savedir = createPath(\"./DATA/PROCESSED/B_PREDICTIONS/BEPIPRED/\")\n",
        "\n",
        "    shannon = rootdir.split(\"/\")[-1].split(\"_\")[-1]\n",
        "    clusterName = basename.split(\"_\")[0]\n",
        "\n",
        "    print(\"Predicting antibody binding for \" + clusterName + \" \" + shannon)\n",
        "    print(\"---------------------------------\" + len(clusterName)*\"-\" + len(shannon)*\"-\")\n",
        "    print(\"---------------------------------\" + len(clusterName)*\"-\" + len(shannon)*\"-\")\n",
        "\n",
        "    BepipredArgs=['python',\n",
        "                  bcell_script,\n",
        "                  '-m Bepipred2',\n",
        "                  '-f',\n",
        "                  f'\"{filepath}\"',\n",
        "                  '>',\n",
        "                  f'\"{savedir}{clusterName}_{shannon}_bpresults.txt\"'\n",
        "                  ]\n",
        "\n",
        "    script=\" \".join(BepipredArgs)\n",
        "    !$script\n",
        "\n",
        "    elapsedtime = time.time() - starttime\n",
        "    print(\"\")\n",
        "    print(\"Predictions for \" + clusterName + \" \" + shannon + \" completed!\")\n",
        "    print(\"Total time: \" + time.strftime(\"%H:%M:%S\", time.gmtime(elapsedtime)))\n",
        "    print(\"-----------------\" + len(clusterName)*\"-\" + len(shannon)*\"-\" + \"-----------\")\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs-hI0L7QaHx"
      },
      "source": [
        "Finally, we run the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mj1PmRURbaq"
      },
      "source": [
        "for root, dirs, files in os.walk(\"./DATA/PROCESSED/B_PREDICTIONS/CONSENSUS/\"):\n",
        "    for f in files:\n",
        "        if f.endswith(\"unmasked.fasta\"):\n",
        "            Bepipred_prediction(os.path.abspath(os.path.join(root,f)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJAGzDq9fjbU"
      },
      "source": [
        "#### **Parsing the output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vkGarQEbPVk"
      },
      "source": [
        "def getUnmaskedAlignment(input_file, output_file):\n",
        "    muscle_exe = r\"/usr/local/bin/muscle3.8.31_i86linux64\"\n",
        "    muscle_cline = MuscleCommandline(muscle_exe, input = input_file, out = output_file, clwstrict=True)\n",
        "    muscle_cline()\n",
        "    \n",
        "    alignment = AlignIO.read(output_file, \"clustal\")\n",
        "    \n",
        "    for record in alignment:\n",
        "        if record.id.endswith(\"unmasked\"):\n",
        "            alignedSeq = str(record.seq)\n",
        "        else:\n",
        "            baseSeq = str(record.seq)\n",
        "\n",
        "    return alignedSeq\n",
        "\n",
        "def BepiParse(results_file, bp_threshold=0.5):\n",
        "    basename = os.path.basename(results_file)\n",
        "    clusterName = basename.split(\"_\")[0]\n",
        "    shannon = basename.split(\"_\")[1]\n",
        "    \n",
        "    output_parsed = f\"./DATA/PROCESSED/B_PREDICTIONS/BEPIPRED/{clusterName}_{shannon}_bepiparsed.fasta\"\n",
        "    open(output_parsed,\"w+\").close()\n",
        "    \n",
        "    bepipred_dict = {}\n",
        "    with open(results_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"input\"):\n",
        "                seqFlag = False\n",
        "                currentID = line.strip()[7:]\n",
        "                bepipred_dict[currentID] = []\n",
        "            elif line.startswith(\"Position\"):\n",
        "                seqFlag = True\n",
        "            else:\n",
        "                if seqFlag:\n",
        "                    lineTuple = tuple(line.split())\n",
        "                    bepipred_dict[currentID].append(lineTuple)\n",
        "    \n",
        "    #We need the masked and unmasked sequences of each protein\n",
        "    masked_file = f\"./DATA/PROCESSED/B_PREDICTIONS/CONSENSUS/SHANNON_{shannon}/{clusterName}_fullconsensus{shannon}.fasta\"   \n",
        "    masked_dict = {record.id:str(record.seq) for record in SeqIO.parse(masked_file,\"fasta\")}\n",
        "\n",
        "    unmasked_file = f\"./DATA/PROCESSED/B_PREDICTIONS/CONSENSUS/SHANNON_{shannon}/{clusterName}_fullconsensus{shannon}_unmasked.fasta\"\n",
        "    unmasked_dict = {record.id:str(record.seq) for record in SeqIO.parse(unmasked_file,\"fasta\")}\n",
        "\n",
        "    #Since the lengths of masked and unmasked proteins might be different, we need\n",
        "    #to align them first and get the unmasked alignment.\n",
        "    for protein, values in bepipred_dict.items():\n",
        "        muscle_input = \"./DATA/PROCESSED/B_PREDICTIONS/BEPIPRED/muscletemp_i.fasta\"\n",
        "        muscle_output = \"./DATA/PROCESSED/B_PREDICTIONS/BEPIPRED/muscletemp_o.fasta\"\n",
        "        \n",
        "        with open(muscle_input, \"w+\") as f:\n",
        "            f.write(\">_masked\\n\")\n",
        "            f.write(masked_dict[protein].replace(\"*\", \"X\").replace(\"-\", \"X\") + \"\\n\")\n",
        "            f.write(\">_unmasked\\n\")\n",
        "            f.write(unmasked_dict[protein] + \"\\n\")\n",
        "\n",
        "\n",
        "        unmaskedSeq = getUnmaskedAlignment(muscle_input, muscle_output)\n",
        "        \n",
        "        parsedSeq = \"\"\n",
        "        \n",
        "        #We have two counters, one for the Bepipred results and the other for\n",
        "        #the protein sequence. \n",
        "        bpred_position = 0\n",
        "        for position, aa in enumerate(unmaskedSeq):\n",
        "            #In case the position is a gap. Bepipred counter does not move, since\n",
        "            #there are no gaps there.\n",
        "            if aa == \"-\":\n",
        "                parsedSeq += aa\n",
        "                continue\n",
        "\n",
        "            #Check if the amino acids match, otherwise something wrong happened.\n",
        "            elif not aa == values[bpred_position][1]:\n",
        "                raise Exception(\"BepiPred results don't match unmasked sequence!\")   \n",
        "\n",
        "            #Amino acids match, but the position is masked.\n",
        "            #Both counters move one position.  \n",
        "            elif masked_dict[protein][position] in [\"*\", \"-\"]:\n",
        "                parsedSeq += masked_dict[protein][position]\n",
        "                bpred_position += 1\n",
        "            \n",
        "            #Amino acids match, now let's see what BepiPred predicts.\n",
        "            #Both counters move one position. \n",
        "            elif float(values[bpred_position][2]) >= bp_threshold:\n",
        "                parsedSeq += aa\n",
        "                bpred_position += 1\n",
        "            else:\n",
        "                parsedSeq += \"_\"\n",
        "                bpred_position += 1\n",
        "        \n",
        "        #Length of the parsed sequence should be the same as the unmasked seq.                        \n",
        "        if not len(parsedSeq) == len(masked_dict[protein]):\n",
        "            raise Exception(\"Parsed length does not match unmasked sequence\")\n",
        "              \n",
        "        os.remove(muscle_input)\n",
        "        os.remove(muscle_output)\n",
        "            \n",
        "        with open(output_parsed,\"a+\") as f:\n",
        "            f.write(\">\" + protein + \"\\n\")\n",
        "            f.write(parsedSeq + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HydnfCAxQYFx"
      },
      "source": [
        "for f in os.listdir(\"./DATA/PROCESSED/B_PREDICTIONS/BEPIPRED\"):\n",
        "    if f.endswith(\"bpresults.txt\"):\n",
        "        BepiParse(f\"./DATA/PROCESSED/B_PREDICTIONS/BEPIPRED/{f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzL4lkS45NKt"
      },
      "source": [
        "### **Generate peptides of >=15 amino acids from prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWuLy51PXm8T"
      },
      "source": [
        "def fetchUniprotName(protein):\n",
        "    uniprotID = protein.split(\"_\")[2]\n",
        "    url = 'https://www.uniprot.org/uniprot/' + uniprotID\n",
        "    path_name = '//*[@id=\"content-protein\"]/h1'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        byte_data = response.content\n",
        "        source_code = html.fromstring(byte_data)\n",
        "        tree = source_code.xpath(path_name)\n",
        "\n",
        "        if not tree:\n",
        "            return [protein, \"NA\"]\n",
        "        else:\n",
        "            return [protein, str(tree[0].text_content())]\n",
        "    except:\n",
        "        return [protein, \"NA\"]\n",
        "\n",
        "def Uniproter(protlist):    \n",
        "    unique_prots = list(set(protlist))\n",
        "    uniprotDict = {}\n",
        "    results = ThreadPool(20).imap_unordered(fetchUniprotName, unique_prots)\n",
        "    for r in results:\n",
        "        uniprotDict[r[0]] = r[1]\n",
        "    return uniprotDict\n",
        "\n",
        "def B_epitopes(bepiparsed_file):\n",
        "\n",
        "    bepipredDict = {str(record.id) : str(record.seq) for record in SeqIO.parse(bepiparsed_file, \"fasta\")}\n",
        "\n",
        "    epitopes_list = []\n",
        "    for id, sequence in bepipredDict.items():\n",
        "        fragments = sequence.replace(\"-\", \" \").replace(\"*\", \" \").replace(\"X\", \" \").replace(\"_\", \" \").split()\n",
        "        for fragment in fragments:\n",
        "            if len(fragment) >= 15:\n",
        "                epitopes_list.append((fragment, id))\n",
        "    \n",
        "    destPath= createPath(\"./DATA/PROCESSED/B_PREDICTIONS/EPITOPES/SEQUENCE_BASED/\")\n",
        "    group = bepiparsed_file.split(\"/\")[-1].split(\"_\")[0]\n",
        "    shannon = bepiparsed_file.split(\"/\")[-1].split(\"_\")[1]\n",
        "    filename = f\"{group}_{shannon}_Bseq.xlsx\"\n",
        "\n",
        "    df = pd.DataFrame(epitopes_list, columns = [\"epitope\", \"protein\"])\n",
        "\n",
        "    uniprotNames = Uniproter(df[\"protein\"])\n",
        "    df[\"protein name\"] = df[\"protein\"].map(uniprotNames)\n",
        "\n",
        "    df.to_csv(destPath + filename)\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kys9-pPe5NKu"
      },
      "source": [
        "bepiparsedList = [f\"./DATA/PROCESSED/B_PREDICTIONS/BEPIPRED/{f}\" for f in os.listdir(\"./DATA/PROCESSED/B_PREDICTIONS/BEPIPRED\") if f.endswith(\".fasta\")]\n",
        "for f in bepiparsedList:\n",
        "    B_epitopes(f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}